{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT NLP - Tokenizers.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNO/SmFcciJ9Elw+tNi0ZE7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/ds-bert/blob/main/BERT%20NLP%20-%20Tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT NLP - Tokenizers**"
      ],
      "metadata": {
        "id": "cb2q3xX_7wEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A tokenizer is in charge of preparing the inputs for a model. The library contains tokenizers for all the models. Most of the tokenizers are available in two flavors: a full python implementation and a “Fast” implementation based on the Rust library."
      ],
      "metadata": {
        "id": "XPxHTYd7-fd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Install Transformers**"
      ],
      "metadata": {
        "id": "X-l_VZTR73iT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "IgezeVBd62Wb"
      },
      "outputs": [],
      "source": [
        "%pip install -qq transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Tokenizer**"
      ],
      "metadata": {
        "id": "p5xhGGP87-Fl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer"
      ],
      "metadata": {
        "id": "UCFboaMJ8C34"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**\n"
      ],
      "metadata": {
        "id": "hOsFOSM4Eyb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")"
      ],
      "metadata": {
        "id": "0qVjauBj8Uk3"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apart from tokens for each words the tokenizer also returns 2 additional tokens to represents **start**, `[CLS]`, **101**, and **end**, `[SEP]`, **102**,  of each sentences. \n",
        "\n",
        "**Returns**\n",
        "- **input_ids:** The input ids are often the only required parameters to be passed to the model as input. They are token indices, numerical representations of tokens building the sequences that will be used as input by the model.\n",
        "\n",
        "- **attention_mask:** This argument indicates to the model which tokens should be attended to, and which should not. Mask to avoid performing attention on padding token indices. Mask values selected in [0, 1]. *1 for tokens that are not masked, 0 for tokens that are masked.*\n",
        "\n",
        "- **token_type_ids:** These require two different sequences to be joined in a single “input_ids” entry, which usually is performed with the help of special tokens, such as the classifier ([CLS]) and separator ([SEP]) tokens. For example, the BERT model builds its two sequence input as such: `[CLS] SEQUENCE_A [SEP] SEQUENCE_B [SEP]`"
      ],
      "metadata": {
        "id": "hd_PmJ9bEQnm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "texts = [\"This is a sentence.\", \"Here is another sentence. This is a little longer.\", \"This is short.\"]\n",
        "tokenizer_ = tokenizer(texts)\n",
        "print(tokenizer_)\n",
        "print(\"input_ids: \", tokenizer_.input_ids)\n",
        "print(\"token_type_ids: \", tokenizer_.token_type_ids)\n",
        "print(\"attention_mask: \", tokenizer_.attention_mask)\n",
        "print(\"decode: \", tokenizer.decode(tokenizer_[\"input_ids\"][0]))\n",
        "print(\"decode: \", tokenizer.decode(tokenizer_[\"input_ids\"][1]))\n",
        "print(\"decode: \", tokenizer.decode(tokenizer_[\"input_ids\"][2]))\n",
        "print(\"decode: \", tokenizer.decode(tokenizer_[\"input_ids\"][0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f2Acy5SuEU5f",
        "outputId": "d49aadc4-6be6-4478-cca7-2c59803bea33"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 2023, 2003, 1037, 6251, 1012, 102], [101, 2182, 2003, 2178, 6251, 1012, 2023, 2003, 1037, 2210, 2936, 1012, 102], [101, 2023, 2003, 2460, 1012, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]}\n",
            "input_ids:  [[101, 2023, 2003, 1037, 6251, 1012, 102], [101, 2182, 2003, 2178, 6251, 1012, 2023, 2003, 1037, 2210, 2936, 1012, 102], [101, 2023, 2003, 2460, 1012, 102]]\n",
            "token_type_ids:  [[0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0]]\n",
            "attention_mask:  [[1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1]]\n",
            "decode:  [CLS] this is a sentence. [SEP]\n",
            "decode:  [CLS] here is another sentence. this is a little longer. [SEP]\n",
            "decode:  [CLS] this is short. [SEP]\n",
            "decode:  [CLS] this is a sentence. [SEP]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer Functions**\n",
        "- **tokenize** - Converts a string in a sequence of tokens, using the tokenizer. Returns list of tokens.\n",
        "- **convert_tokens_to_ids** - Return ids of the tokens.\n",
        "- **convert_ids_to_tokens** -  Convert token ids to token or word.\n",
        "- **convert_tokens_to_string** - Returns string from the tokens."
      ],
      "metadata": {
        "id": "xwWxeU6B9BK2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_(text):\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  token_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "  words = tokenizer.convert_ids_to_tokens(token_ids)\n",
        "  string = tokenizer.convert_tokens_to_string(tokens)\n",
        "  print(\"tokenize: \", tokens, \"\\t\", \"Length\", len(tokens))\n",
        "  print(\"convert_tokens_to_ids: \", token_ids, \"\\t\", \"Length\", len(token_ids))\n",
        "  print(\"convert_ids_to_tokens: \", words, \"\\t\", \"Length\", len(words))\n",
        "  print(\"convert_tokens_to_string: \", string)"
      ],
      "metadata": {
        "id": "2KJdAqX_AX_F"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1:** Plain Text"
      ],
      "metadata": {
        "id": "7d4Zh8YZ_ezE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example\"\n",
        "tokenize_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXALJkCw_hkh",
        "outputId": "056ba32e-a30a-4d3e-b545-0b8152a8f38a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize:  ['this', 'is', 'a', 'simple', 'text', 'example'] \t Length 6\n",
            "convert_tokens_to_ids:  [2023, 2003, 1037, 3722, 3793, 2742] \t Length 6\n",
            "convert_ids_to_tokens:  ['this', 'is', 'a', 'simple', 'text', 'example'] \t Length 6\n",
            "convert_tokens_to_string:  this is a simple text example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2:** Comma Seperated"
      ],
      "metadata": {
        "id": "MWOFdva4BXK3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example, this is alos an example\"\n",
        "tokenize_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXslyMUQBY_z",
        "outputId": "28726690-a8c0-4729-97e8-287429b55622"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize:  ['this', 'is', 'a', 'simple', 'text', 'example', ',', 'this', 'is', 'al', '##os', 'an', 'example'] \t Length 13\n",
            "convert_tokens_to_ids:  [2023, 2003, 1037, 3722, 3793, 2742, 1010, 2023, 2003, 2632, 2891, 2019, 2742] \t Length 13\n",
            "convert_ids_to_tokens:  ['this', 'is', 'a', 'simple', 'text', 'example', ',', 'this', 'is', 'al', '##os', 'an', 'example'] \t Length 13\n",
            "convert_tokens_to_string:  this is a simple text example , this is alos an example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3:** Special Characters"
      ],
      "metadata": {
        "id": "xL2tDkgZBnDz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example, (this is alos an example)\"\n",
        "tokenize_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdbZh07yBoqt",
        "outputId": "ed8cce59-abed-45de-9f96-acdaf9c32c1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize:  ['this', 'is', 'a', 'simple', 'text', 'example', ',', '(', 'this', 'is', 'al', '##os', 'an', 'example', ')'] \t Length 15\n",
            "convert_tokens_to_ids:  [2023, 2003, 1037, 3722, 3793, 2742, 1010, 1006, 2023, 2003, 2632, 2891, 2019, 2742, 1007] \t Length 15\n",
            "convert_ids_to_tokens:  ['this', 'is', 'a', 'simple', 'text', 'example', ',', '(', 'this', 'is', 'al', '##os', 'an', 'example', ')'] \t Length 15\n",
            "convert_tokens_to_string:  this is a simple text example , ( this is alos an example )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 4:** Two Sentences."
      ],
      "metadata": {
        "id": "mqwu4saBCOUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example. (This is alos an example)\"\n",
        "tokenize_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq_LAVdUCV95",
        "outputId": "ecd12b76-d1ef-4203-b60f-53b9d6ab3e7f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokenize:  ['this', 'is', 'a', 'simple', 'text', 'example', '.', '(', 'this', 'is', 'al', '##os', 'an', 'example', ')'] \t Length 15\n",
            "convert_tokens_to_ids:  [2023, 2003, 1037, 3722, 3793, 2742, 1012, 1006, 2023, 2003, 2632, 2891, 2019, 2742, 1007] \t Length 15\n",
            "convert_ids_to_tokens:  ['this', 'is', 'a', 'simple', 'text', 'example', '.', '(', 'this', 'is', 'al', '##os', 'an', 'example', ')'] \t Length 15\n",
            "convert_tokens_to_string:  this is a simple text example . ( this is alos an example )\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encode**\n",
        "\n",
        "Converts a string to a sequence of ids (integer), using the tokenizer and vocabulary. Same as doing `self.convert_tokens_to_ids(self.tokenize(text))`.\n",
        "\n",
        "While this function is indeed useful, it does have a limitation: it can only process one string. In other words, it does not support batches. Therefore, to see the result of the function, we need to employ a for loop. As we can see above we can pass a list to **tokenizer** without using any loops unlike **encode** which will need loops.\n",
        "\n",
        "**max_length, padding, truncation:** Optional Parameters"
      ],
      "metadata": {
        "id": "KIj3aHaDC6zf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"This is a sentence.\", \"Here is another sentence. This is a little longer.\", \"This is short.\"]\n",
        "for sentence in sentences:\n",
        "  print(tokenizer.encode(sentence))\n",
        "  print(tokenizer.encode(sentence, max_length=12, padding=\"max_length\", truncation=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TAXVrPyqLr1j",
        "outputId": "2f127282-24ff-466a-dc0a-18694f2761b6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2023, 2003, 1037, 6251, 1012, 102]\n",
            "[101, 2023, 2003, 1037, 6251, 1012, 102, 0, 0, 0, 0, 0]\n",
            "[101, 2182, 2003, 2178, 6251, 1012, 2023, 2003, 1037, 2210, 2936, 1012, 102]\n",
            "[101, 2182, 2003, 2178, 6251, 1012, 2023, 2003, 1037, 2210, 2936, 102]\n",
            "[101, 2023, 2003, 2460, 1012, 102]\n",
            "[101, 2023, 2003, 2460, 1012, 102, 0, 0, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def encode_(text):\n",
        "  tokens = tokenizer.encode(text)\n",
        "  tokens_op = tokenizer.encode(text, max_length=12, padding=\"max_length\", truncation=True)\n",
        "  print(tokens)\n",
        "  print(tokens_op)"
      ],
      "metadata": {
        "id": "paKpqgsJC8p2"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1:** Plain Text"
      ],
      "metadata": {
        "id": "iu-WQrzGDiWm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example\"\n",
        "encode_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04eb2593-6c37-4529-aab3-c306cceb2fb9",
        "id": "Lq7XBKYlDiWn"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 102]\n",
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 102, 0, 0, 0, 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2:** Comma Seperated"
      ],
      "metadata": {
        "id": "8BLF58ubDiWn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example, this is alos an example\"\n",
        "encode_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "45ab1431-18f8-4ce7-b527-ca2f152b37e6",
        "id": "yaCssbkFDiWo"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 1010, 2023, 2003, 2632, 2891, 2019, 2742, 102]\n",
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 1010, 2023, 2003, 2632, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3:** Special Characters"
      ],
      "metadata": {
        "id": "7_rMQBEtDiWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example, (this is alos an example)\"\n",
        "encode_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b03530d-7571-460b-c72c-21f456b38efa",
        "id": "3zARscOpDiWo"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 1010, 1006, 2023, 2003, 2632, 2891, 2019, 2742, 1007, 102]\n",
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 1010, 1006, 2023, 2003, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 4:** Two Sentences."
      ],
      "metadata": {
        "id": "Rd6iY8HEDiWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example. (This is alos an example)\"\n",
        "encode_(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a174b11-5aa1-4d9e-af01-33fb427bd328",
        "id": "olo63wvCDiWo"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 1012, 1006, 2023, 2003, 2632, 2891, 2019, 2742, 1007, 102]\n",
            "[101, 2023, 2003, 1037, 3722, 3793, 2742, 1012, 1006, 2023, 2003, 102]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**encode_plus**\n",
        "\n",
        "`tokenizer.encode_plus()` is actually quite similar to the regular encode function, except that it returns a dictionary that includes all the keys that we’ve discussed above: **input_ids**, **token_type_ids**, and **attention_mask**.\n",
        "\n",
        "Much like ``tokenizer.encode()``, the same arguments, **maximum length, padding, and truncation**, equally apply."
      ],
      "metadata": {
        "id": "3qb7oJdALctl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"This is a sentence.\", \"Here is another sentence. This is a little longer.\", \"This is short.\"]\n",
        "for sentence in sentences:\n",
        "    print(tokenizer.encode_plus(sentence))\n",
        "    print(tokenizer.encode_plus(sentence, max_length=12, padding=\"max_length\", truncation=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UX7TmCzMm0r",
        "outputId": "71eb993e-df16-455c-b332-1c626e5b88a3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [101, 2023, 2003, 1037, 6251, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 2023, 2003, 1037, 6251, 1012, 102, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0]}\n",
            "{'input_ids': [101, 2182, 2003, 2178, 6251, 1012, 2023, 2003, 1037, 2210, 2936, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 2182, 2003, 2178, 6251, 1012, 2023, 2003, 1037, 2210, 2936, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 2023, 2003, 2460, 1012, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}\n",
            "{'input_ids': [101, 2023, 2003, 2460, 1012, 102, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**batch_encode_plus**\n",
        "\n",
        "The encoding functions we have looked so far all expected a string as input. But normally, the input would come in batches, and we don’t want to use a for loop to encode each, append them to some result list, and et cetera. `tokenizer.batch_encode_plus()`, as the name implies, is a function that can handle batch inputs."
      ],
      "metadata": {
        "id": "KJJbKO5cNTwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"This is a sentence.\", \"Here is another sentence. This is a little longer.\", \"This is short.\"]\n",
        "print(tokenizer.batch_encode_plus(sentence))\n",
        "print(tokenizer.batch_encode_plus(sentence, max_length=12, padding=\"max_length\", truncation=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GD0mEomMNrIy",
        "outputId": "73e9b1b1-2ef2-4936-f42c-00fa4d2afeca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': [[101, 1056, 102], [101, 1044, 102], [101, 1045, 102], [101, 1055, 102], [101, 102], [101, 1045, 102], [101, 1055, 102], [101, 102], [101, 1055, 102], [101, 1044, 102], [101, 1051, 102], [101, 1054, 102], [101, 1056, 102], [101, 1012, 102]], 'token_type_ids': [[0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0], [0, 0, 0], [0, 0, 0], [0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0], [0, 0, 0]], 'attention_mask': [[1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1], [1, 1, 1], [1, 1, 1], [1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1], [1, 1, 1]]}\n",
            "{'input_ids': [[101, 1056, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1044, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1045, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1055, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1044, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1051, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1054, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1056, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0], [101, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Special Tokens**\n",
        "\n",
        "For our experiment, we need to know what BERT’s special tokens are. Specifically, we have to know what the mask token looks like in order to conduct some basic masked language modeling task."
      ],
      "metadata": {
        "id": "eyjb2DcWO1V6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZesgXIIO7vz",
        "outputId": "e31a0b8f-ffd3-45a6-8d86-8b3f1037438f"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'cls_token': '[CLS]',\n",
              " 'mask_token': '[MASK]',\n",
              " 'pad_token': '[PAD]',\n",
              " 'sep_token': '[SEP]',\n",
              " 'unk_token': '[UNK]'}"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tensorflow**\n",
        "\n",
        "We can load the tokenizer or pre processor from Tensorflow hub. It is also available as a KerasLayer. It takes a list as an input.\n",
        "\n",
        "**Returns**\n",
        "The tokenizer returns a dictionary with three important itmes:\n",
        "\n",
        "The tokenizer returns a dictionary with three important itmes:\n",
        "\n",
        "- **input_word_ids:** Are the indices corresponding to each token in the sentence. Tensor of shape [batch_size, seq_length] with the token ids of the packed input sequence (that is, including a *start-of-sequence token, end-of-segment tokens, and padding*).\n",
        "\n",
        "- **input_mask:** Indicates whether a token should be attended to or not. Tensor of shape [batch_size, seq_length] with value **1** at the position of all input tokens present before padding and value **0** for the padding tokens.\n",
        "\n",
        "- **input_type_ids:** Identifies which sequence a token belongs to when there is more than one sequence. Tensor of shape [batch_size, seq_length] with the index of the input segment that gave rise to the input token at the respective position. The *first input segment (index 0) includes the start-of-sequence token and its end-of-segment token. The second and later segments (if present) include their respective end-of-segment token. Padding tokens get index 0 again.*"
      ],
      "metadata": {
        "id": "-LDoXBG3XPMS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qq tensorflow_hub\n",
        "%pip install -qq tensorflow_text"
      ],
      "metadata": {
        "id": "1faRb0sLXUaA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "HBfDBT2tXaxv"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/1')"
      ],
      "metadata": {
        "id": "0w8qVtlLYHJk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 1:** Plain Text"
      ],
      "metadata": {
        "id": "dNvmvGMgYQpN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example\"\n",
        "preprocess([text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23f86137-b356-4411-c72d-c8d6723a973e",
        "id": "0e_RyXeAYQpN"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[ 101, 2023, 2003, 1037, 3722, 3793, 2742,  102,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 2:** Comma Seperated"
      ],
      "metadata": {
        "id": "aKpg7BKDYQpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example, this is alos an example\"\n",
        "preprocess([text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d0973ab5-1136-40c9-885b-be955d92e00f",
        "id": "huw17f_8YQpO"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[ 101, 2023, 2003, 1037, 3722, 3793, 2742, 1010, 2023, 2003, 2632,\n",
              "         2891, 2019, 2742,  102,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 3:** Special Characters"
      ],
      "metadata": {
        "id": "nVpxIBoDYQpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example, (this is alos an example)\"\n",
        "preprocess([text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aeed8aa1-5700-49df-ed80-29f80975dfb1",
        "id": "6V4S-3VbYQpO"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[ 101, 2023, 2003, 1037, 3722, 3793, 2742, 1010, 1006, 2023, 2003,\n",
              "         2632, 2891, 2019, 2742, 1007,  102,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example 4:** Two Sentences."
      ],
      "metadata": {
        "id": "nAjzgdatYQpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a simple text example. (This is alos an example)\"\n",
        "preprocess([text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72fb44be-aa38-4ca0-9b38-71db0a65632b",
        "id": "DVHUAVAuYQpO"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[ 101, 2023, 2003, 1037, 3722, 3793, 2742, 1012, 1006, 2023, 2003,\n",
              "         2632, 2891, 2019, 2742, 1007,  102,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"This is a sentence.\", \"Here is another sentence. This is a little longer.\", \"This is short.\"]\n",
        "preprocess(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HvcJ6KF3ZQ0s",
        "outputId": "d9eaa9f6-072e-4226-eb94-0c04cd05feb6"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(3, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(3, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(3, 128), dtype=int32, numpy=\n",
              " array([[ 101, 2023, 2003, 1037, 6251, 1012,  102,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0],\n",
              "        [ 101, 2182, 2003, 2178, 6251, 1012, 2023, 2003, 1037, 2210, 2936,\n",
              "         1012,  102,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0],\n",
              "        [ 101, 2023, 2003, 2460, 1012,  102,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
              "            0,    0,    0,    0,    0,    0,    0]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    }
  ]
}