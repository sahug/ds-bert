{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/ds-bert/blob/main/BERT%20NLP%20-%20Session%202%20-%20Introduction%20-%20Sentiment%20Classification%20Using%20BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyDVvzKiuk6D"
      },
      "source": [
        "**BERT NLP - Session 2 - Sentiment Classification Using BERT**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iIRieKahuk6F"
      },
      "source": [
        "**Problem Statement**\n",
        "We will use the IMDB Movie Reviews Dataset, where based on the given review we have to classify the sentiments of that particular review like positive or negative.\n",
        "\n",
        "![image.png](attachment:image.png)\n",
        "\n",
        "**Introduction**\n",
        "Chatbots, virtual assistants, and dialog agents will typically classify queries into specific intents in order to generate the most coherent response.\n",
        "\n",
        "Intent classification is a classification problem that predicts the intent label for any given user query. It is usually a multi-class classification problem, where the query is assigned one unique label.\n",
        "\n",
        "For example, the query `“how much does the limousine service cost within Pune”` is labeled as `“groundfare”` while the query `“what kind of ground transportation is available in Pune”` is labeled as `“ground_service”`. The query `“I want to fly from Delhi at 8:38 am and arrive in Pune at 11:10 in the morning”` is a `“flight”` intent, while `“show me the costs and times for flights from Delhi to Pune”` is an `“airfare+flight_time”` intent.\n",
        "\n",
        "The examples mentioned above shows how ambiguous intent labeling can be. Any addition of misleading words, causing multiple intents to be present in the same query. Attention-based learning methods were proposed for intent classification (Liu and Lane, 2016; Goo et al., 2018). One type of network built with attention is called a Transformer. It applies attention mechanisms to gather information about the relevant context of a given word, and then encode that context in a rich vector that smartly represents the word.\n",
        "\n",
        "**What is transformer?**\n",
        "Before digging into the concepts of BERT, let us understand why Transformers and why it is needed?\n",
        "\n",
        "To understand transformers we first must understand the attention mechanism.\n",
        "\n",
        "The Attention mechanism enables the transformers to have extremely long term memory. A transformer model can “attend” or “focus” on all previous tokens that have been generated.\n",
        "\n",
        "Recurrent neural networks (RNN) are also capable of looking at previous inputs too. But the power of the attention mechanism is that it doesn’t suffer from short term memory. RNNs can theoretically access information arbitrarily far in the past, but in practice, they have a hard time keeping that information in their internal state (this is related to the vanishing gradient problem). This is still true for Gated Recurrent Units (GRU’s) and Long-short Term Memory (LSTM’s) networks, although they do a bigger capacity to achieve longer-term memory, therefore, having a longer window to reference from. The attention mechanism, in theory, and given enough compute resources, have an infinite window to reference from, therefore being capable of using the entire context of the story while generating the text.\n",
        "\n",
        "![image-2.png](attachment:image-2.png)\n",
        "\n",
        "**What is BERT?**\n",
        "Bidirectional Encoder Representations from Transformers (BERT) is a technique for NLP (Natural Language Processing) pre-training developed by Google. BERT was created and published in 2018 by Jacob Devlin and his colleagues from Google. Google is leveraging BERT to better understand user searches.\n",
        "\n",
        "BERT is designed to pre-train deep bidirectional representations from an unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be finetuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications.\n",
        "\n",
        "![image-3.png](attachment:image-3.png)\n",
        "\n",
        "**Key Points to Remember**\n",
        " - BERT is basically a trained Transformer Encoder stack, with twelve in the Base version, and twenty-four in the Large version, compared to 6 encoder layers in the original Transformer.\n",
        " - BERT encoders have larger feedforward networks (768 and 1024 nodes in Base and Large respectively) and more attention heads (12 and 16 respectively). BERT was trained on Wikipedia and Book Corpus, a dataset containing +10,000 books of different genres.\n",
        " \n",
        "Below you can see a diagram of additional variants of BERT pre-trained on specialized corpora.\n",
        "\n",
        "![image-4.png](attachment:image-4.png)\n",
        "\n",
        "here are two existing strategies for applying pre-trained language representations to downstream tasks: feature-based and fine-tuning.\n",
        "\n",
        "The feature-based approach, such as ELMo (Peters et al., 2018a), uses task-specific architectures that include the pre-trained representations as additional features. The fine-tuning approach, such as the Generative Pre-trained Transformer (OpenAI GPT) (Radford et al., 2018), introduces minimal task-specific parameters and is trained on the downstream tasks by simply fine-tuning all pre-trained parameters.\n",
        "\n",
        "The two approaches share the same objective function during pre-training, where they use unidirectional language models to learn general language representations.\n",
        "\n",
        "**Why BERT?**\n",
        "Proper language representation is key for general-purpose language understanding by machines. Context-free models such as word2vec or GloVe generate a single word embedding representation for each word in the vocabulary. For example, the word “bank” would have the same representation in “bank deposit” and in “riverbank”.\n",
        "\n",
        "Contextual models instead generate a representation of each word that is based on the other words in the sentence. BERT, as a contextual model, captures these relationships in a bidirectional way. BERT was built upon recent work and clever ideas in pre-training contextual representations including Semi-supervised Sequence Learning, Generative Pre-Training, ELMo, the OpenAI Transformer, ULMFit, and the Transformer. Although these models are all unidirectional or shallowly bidirectional, BERT is fully bidirectional.\n",
        "\n",
        "We will use BERT to extract high-quality language features from the ATIS query text data, and fine-tune BERT on a specific task (classification) with its own data to produce a state of the art predictions.\n",
        "\n",
        "**What is ktrain?**\n",
        "ktrain is a library to help build, train, debug, and deploy neural networks in the deep learning software framework, Keras.\n",
        "\n",
        "ktrain uses tf.keras in TensorFlow instead of standalone Keras.) Inspired by the fastai library, with only a few lines of code, ktrain allows you to easily:\n",
        "\n",
        "estimate an optimal learning rate for your model given your data using a learning rate finder\n",
        "employ learning rate schedules such as the triangular learning rate policy, 1cycle policy, and SGDR to more effectively train your model\n",
        "employ fast and easy-to-use pre-canned models for both text classification (e.g., NBSVM, fastText, GRU with pre-trained word embeddings) and image classification (e.g., ResNet, Wide Residual Networks, Inception)\n",
        "load and preprocess text and image data from a variety of formats\n",
        "inspect data points that were misclassified to help improve your model\n",
        "leverage a simple prediction API for saving and deploying both models and data-preprocessing steps to make predictions on new raw data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -q ktrain"
      ],
      "metadata": {
        "id": "tm5V-6ASvBLI",
        "outputId": "de36bb74-e013-4e77-978a-d074052946c9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 25.3 MB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 22.3 MB 85.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 981 kB 46.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 263 kB 56.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 50.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 42.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 468 kB 48.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 880 kB 62.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 52.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 46.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 84 kB 2.7 MB/s \n",
            "\u001b[?25h  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.4 requires scikit-learn>=1.0.0, but you have scikit-learn 0.24.2 which is incompatible.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Bu0ZroHruk6K"
      },
      "outputs": [],
      "source": [
        "import ktrain\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from ktrain import text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kbdTnGjAuk6L"
      },
      "outputs": [],
      "source": [
        "# loading the train dataset\n",
        "\n",
        "data_train = pd.read_excel(\n",
        "    \"data/IMDB-Movie-Reviews-Large-Dataset-50k/train.xlsx\", dtype=str\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rgSdbC_Quk6L"
      },
      "outputs": [],
      "source": [
        "# loading the test dataset\n",
        "\n",
        "data_test = pd.read_excel(\n",
        "    \"data/IMDB-Movie-Reviews-Large-Dataset-50k/test.xlsx\", dtype=str\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xcuUdpHxuk6L"
      },
      "outputs": [],
      "source": [
        "# dimension of the dataset\n",
        "\n",
        "print(\"Size of train dataset: \", data_train.shape)\n",
        "print(\"Size of test dataset: \", data_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KtjjUudCuk6L"
      },
      "source": [
        "#### Observation: \n",
        "Both train and test dataset is having 25000 rows and 2 columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R336qKuduk6M"
      },
      "outputs": [],
      "source": [
        "# printing last rows of train dataset\n",
        "\n",
        "data_train.tail()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiEQyfxvuk6M"
      },
      "outputs": [],
      "source": [
        "# printing head rows of test dataset\n",
        "\n",
        "data_test.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwd3aj9ruk6M"
      },
      "source": [
        "#### Splitting data into Train and Test:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upKPIv8Juk6M"
      },
      "outputs": [],
      "source": [
        "# text.texts_from_df return two tuples\n",
        "# maxlen means it is considering that much words and rest are getting trucated\n",
        "# preprocess_mode means tokenizing, embedding and transformation of text corpus(here it is considering BERT model)\n",
        "\n",
        "\n",
        "(X_train, y_train), (X_test, y_test), preproc = text.texts_from_df(\n",
        "    train_df=data_train,\n",
        "    text_column=\"Reviews\",\n",
        "    label_columns=\"Sentiment\",\n",
        "    val_df=data_test,\n",
        "    maxlen=500,\n",
        "    preprocess_mode=\"bert\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUwCpSXVuk6N"
      },
      "source": [
        "#### Observation:\n",
        "\n",
        " - You can able to see that it is detecting language as an English\n",
        " - Also, this is not a multilabel classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlplmrU0uk6N"
      },
      "outputs": [],
      "source": [
        "# name = \"bert\" means, here we are using BERT model.\n",
        "\n",
        "model = text.text_classifier(\n",
        "    name=\"bert\", train_data=(X_train, y_train), preproc=preproc\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdtQ4NCXuk6N"
      },
      "outputs": [],
      "source": [
        "# here we have taken batch size as 6 as from the documentation it is recommend to use this with maxlen as 500\n",
        "\n",
        "learner = ktrain.get_learner(\n",
        "    model=model, train_data=(X_train, y_train), val_data=(X_test, y_test), batch_size=6\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wQGJJ1wluk6N"
      },
      "outputs": [],
      "source": [
        "# find out best learning rate?\n",
        "# learner.lr_find()\n",
        "# learner.lr_plot()\n",
        "\n",
        "# it may take days or many days to find out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V-CwXUFWuk6N"
      },
      "outputs": [],
      "source": [
        "# Essentially fit is a very basic training loop, whereas fit one cycle uses the one cycle policy callback\n",
        "\n",
        "learner.fit_onecycle(lr=2e-5, epochs=1)\n",
        "\n",
        "predictor = ktrain.get_predictor(learner.model, preproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBN1ISqUuk6O"
      },
      "outputs": [],
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A3cvj6Meuk6O"
      },
      "outputs": [],
      "source": [
        "# sample dataset to test on\n",
        "\n",
        "data = [\n",
        "    \"this movie was horrible, the plot was really boring. acting was okay\",\n",
        "    \"the fild is really sucked. there is not plot and acting was bad\",\n",
        "    \"what a beautiful movie. great plot. acting was good. will see it again\",\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YHGseGNXuk6O"
      },
      "outputs": [],
      "source": [
        "predictor.predict(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JVv8X_Cauk6O"
      },
      "source": [
        "#### Intepretation of above results :\n",
        "\n",
        " - `this movie was horrible, the plot was really boring. acting was okay` – neg\n",
        " - `the fild is really sucked. there is not plot and acting was bad` – neg\n",
        " - `what a beautiful movie. great plot. acting was good. will see it again` – pos\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5WUQSW6fuk6P"
      },
      "outputs": [],
      "source": [
        "# return_proba = True means it will give the prediction probabilty for each class\n",
        "\n",
        "predictor.predict(data, return_proba=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WSZnkBAGuk6P"
      },
      "outputs": [],
      "source": [
        "# classes available\n",
        "\n",
        "predictor.get_classes()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVno24aWuk6P"
      },
      "source": [
        "#### Summary\n",
        " - First, We have loaded the pre-loaded the dataset and process it using the pandas dataframe.\n",
        " - Thereafter we have used pre-trained model weights of BERT on our dataset using kTrain library.\n",
        " - Then, we have found the best learning parameter and using that we have fit the model.\n",
        " - Finally, using that model we have predicted our output.\n",
        "\n",
        "Note : For better performance you can distilBERT that is one variant of BERT only."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "BERT NLP - Session 2 - Sentiment Classification Using BERT.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}