{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT NLP - Session 9 - Step by Step BERT.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOHwJ5vC0FB/WcyYqwNVO2e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/ds-bert/blob/main/BERT%20NLP%20-%20Step%20by%20Step%20BERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT NLP - Session 9 - Step by Step BERT**"
      ],
      "metadata": {
        "id": "S_M7o9GWqOqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**What is BERT?**\n",
        "\n",
        "Bert stands for Bidirectional Encoder Representations from Transformers. Itâ€™s google new techniques for NLP pre-training language representation. Which means now machine learning communities can use Bert models that have been training already on a large number of words,(some researchers say the Bert model train on the English Wikipedia 2,500 million words) for NLP models to do a wide variety of tasks such as Question Answering tasks, Named Entity Recognition (NER), and Classification like sentiment analysis.\n",
        "In Bert paper, they present two types of Bert models one is the Best Base and the other is Bert Large. Both of these models have a large number of encoder layers 12 for the base and 24 for the large. If you understand the concept of transformers. You will see that Bert also trained on the Encoder stacks in the transformers to use the same attention mechanism. But why is it called bidirectional?\n",
        "\n",
        "**What is bidirectional mean?**\n",
        "\n",
        "Because the transformers encoder reads the entire sequence of the words at once which is the opposite of the directional models that read the input sequentially for the left to the right or from the right to the left. The bidirectional method will help the model to learn and understand the meaning and the intention of the word based on its surrounding. Since we will use it for toxic classification, we will explain only the Bert steps for classification tasks only.\n",
        "\n",
        "**What is the input of Bert?**\n",
        "\n",
        "The input of Bert is a special input start with [CLS] token stand for classification. As in the Transformers, Bert will take a sequence of words (vector) as an input that keeps feed up from the first encoder layer up to the last layer in the stack. Each layer in the stack will apply the self-attention method to the sequence after that it will pass to the feed-forward network to deliver the next encoder layer.\n",
        "\n",
        "**What is the output of Bert?**\n",
        "\n",
        "The output of Bert model contains the vector of size (hidden size) and the first position in the output is the [CLS] token. Now, this output can be used as an input to our classifier neural network for classification of the toxicity of the words. In the Bert paper, they achieve a great result by using only a single layer neural network as the classifier.\n"
      ],
      "metadata": {
        "id": "8MJ1vdiAqWyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this cell and select the kaggle.json file downloaded\n",
        "# from the Kaggle account settings page.\n",
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "GAzDDY2er5OG",
        "outputId": "d74aeddf-f574-44ab-de1c-76aaa53ec957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-0cb1163f-1bd3-40fa-83de-d06e45224451\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-0cb1163f-1bd3-40fa-83de-d06e45224451\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"gajendraks\",\"key\":\"cdd74665a04eb627273caac8b44b770d\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's make sure the kaggle.json file is present.\n",
        "!ls -lha kaggle.json"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aSzZxk5kr-EW",
        "outputId": "370de1cf-d895-4d4f-98a8-9a53a65592a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 66 Mar 24 02:07 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Next, install the Kaggle API client.\n",
        "%pip install -qq kaggle"
      ],
      "metadata": {
        "id": "WQnPnVTTr_yQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# The Kaggle API client expects this file to be in ~/.kaggle,\n",
        "# so move it there.\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "# This permissions change avoids a warning on Kaggle tool startup.\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "X4ATeIRYsDha"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List available datasets.\n",
        "!kaggle datasets list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlqeN4j9sHk9",
        "outputId": "f46d8145-bed3-4cab-d945-ee8e1ceafc84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ref                                                                       title                                                size  lastUpdated          downloadCount  voteCount  usabilityRating  \n",
            "------------------------------------------------------------------------  --------------------------------------------------  -----  -------------------  -------------  ---------  ---------------  \n",
            "datasets/piterfm/2022-ukraine-russian-war                                 2022 Ukraine Russia War                               1KB  2022-03-23 09:28:52           2471        170  1.0              \n",
            "datasets/prasertk/healthy-lifestyle-cities-report-2021                    Healthy Lifestyle Cities Report 2021                  2KB  2022-03-03 00:26:02           2787         91  1.0              \n",
            "datasets/prasertk/netflix-daily-top-10-in-us                              Netflix daily top 10                                 70KB  2022-03-12 13:22:19           1237         35  1.0              \n",
            "datasets/vivek468/superstore-dataset-final                                Superstore Dataset                                  550KB  2022-02-17 11:33:07           3871         90  1.0              \n",
            "datasets/equinxx/spotify-top-50-songs-in-2021                             Spotify top 50 songs in 2021                          4KB  2022-03-17 22:59:45           1321         39  1.0              \n",
            "datasets/kamilpytlak/personal-key-indicators-of-heart-disease             Personal Key Indicators of Heart Disease              3MB  2022-02-16 10:18:03           4059         76  1.0              \n",
            "datasets/majyhain/height-of-male-and-female-by-country-2022               Height of Male and Female by Country 2022             4KB  2022-02-02 00:40:19           6988        186  1.0              \n",
            "datasets/pranalibose/amazon-seller-order-status-prediction                Amazon Seller - Order Status Prediction              23KB  2022-02-26 06:31:07           2068         55  0.88235295       \n",
            "datasets/ashishjangra27/ted-talks                                         TED Talks                                           298KB  2022-02-23 15:16:08           2445        108  1.0              \n",
            "datasets/yamaerenay/spotify-dataset-19212020-600k-tracks                  Spotify Dataset 1921-2020, 600k+ Tracks             193MB  2022-03-13 21:12:47            451         27  0.9411765        \n",
            "datasets/ramjasmaurya/exports-and-imports-of-india19972022                Exports and Imports of India(1997-2022)              89KB  2022-03-08 14:30:40            925         23  1.0              \n",
            "datasets/shivkumarganesh/tinder-google-play-store-review                  Tinder Dating App - Google Play Store Review         79MB  2022-03-19 10:32:39            340         31  1.0              \n",
            "datasets/maricinnamon/walt-disney-character-dataset                       Walt Disney Character Dataset                        35KB  2022-03-15 12:51:01            523         28  1.0              \n",
            "datasets/timmofeyy/-cerial-prices-changes-within-last-30-years            ðŸŒ½ Cerial Prices Changes Within Last 30 Years          8KB  2022-03-17 11:46:22            375         16  1.0              \n",
            "datasets/timmofeyy/-coffee-rice-and-beef-price-changes-for-30-years       â˜•Coffee, Rice and Beef Prices Changes for 30 Years    5KB  2022-03-15 10:41:06            571         30  1.0              \n",
            "datasets/minisam/marvel-movie-dataset                                     Marvel Movie dataset                                  9KB  2022-03-02 20:03:37           1480         26  0.9411765        \n",
            "datasets/soumyadiptadas/students-math-score-for-different-teaching-style  Student's math score for different teaching style     2KB  2022-02-23 12:36:06           2713         53  1.0              \n",
            "datasets/danielfesalbon/covid-19-global-reports-early-march-2022          COVID -19 Global Reports early March 2022             2MB  2022-03-05 10:51:39           1084         29  1.0              \n",
            "datasets/meetnagadia/walmart-stock-price-from-19722022                    Walmart Stock Price from 1972-2022                  221KB  2022-03-08 11:13:54            552         28  1.0              \n",
            "datasets/vinitshah0110/food-composition                                   Food Composition                                    159KB  2022-03-14 05:04:39            779         39  1.0              \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J9ll_JxBs_oc",
        "outputId": "17ae43ab-7eb4-473c-83e0-04e4fa2eb0b8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading jigsaw-multilingual-toxic-comment-classification.zip to /content\n",
            " 99% 1.07G/1.08G [00:11<00:00, 93.5MB/s]\n",
            "100% 1.08G/1.08G [00:11<00:00, 99.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "OybrNHyjqwGv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qq transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v0ELpAg4q_1l",
        "outputId": "d1a3b1cf-2c68-4802-8374-48f0dac18746"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.17.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.63.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.5)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.49)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.11.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.6.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.7.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle competitions download -c jigsaw-multilingual-toxic-comment-classification"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emVbrGyurbhH",
        "outputId": "21919a91-2784-447b-eb4c-a500d0a897ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/kaggle\", line 5, in <module>\n",
            "    from kaggle.cli import main\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/__init__.py\", line 23, in <module>\n",
            "    api.authenticate()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/kaggle/api/kaggle_api_extended.py\", line 166, in authenticate\n",
            "    self.config_file, self.config_dir))\n",
            "OSError: Could not find kaggle.json. Make sure it's located in /root/.kaggle. Or use the environment method.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import transformers\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from tqdm.notebook import tqdm\n",
        "from tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors"
      ],
      "metadata": {
        "id": "_5AdSa9iqBdu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile(\"/content/jigsaw-multilingual-toxic-comment-classification.zip\",\"r\") as zip_ref:\n",
        "    zip_ref.extractall(\"jigsaw-multilingual-toxic-comment-classification\")"
      ],
      "metadata": {
        "id": "Ps-y0OwbvY5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function for Encoding the comment**\n",
        "\n",
        "Encode job is to convert word into vector encapsulate the meaning of the word, similar word has a closer number."
      ],
      "metadata": {
        "id": "6AhnMWy-wlgT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def regular_encode(texts, tokenizer, maxlen=512):\n",
        "    \"\"\"\n",
        "    Function to encode the word\n",
        "    \"\"\"\n",
        "    # encode the word to vector of integer\n",
        "    enc_di = tokenizer.batch_encode_plus(\n",
        "        texts, \n",
        "        return_token_type_ids=False,\n",
        "        pad_to_max_length=True,\n",
        "        max_length=maxlen\n",
        "    )\n",
        "    \n",
        "    return np.array(enc_di['input_ids'])"
      ],
      "metadata": {
        "id": "xkHK08VKweFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Function for build Keras model**"
      ],
      "metadata": {
        "id": "TWPWLyfUwu97"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_model(transformer, max_len=512):\n",
        "\n",
        "    \"\"\"\n",
        "    This function to build and compile Keras model\n",
        "    \"\"\"\n",
        "    #Input: for define input layer\n",
        "    #shape is vector with 512-dimensional vectors\n",
        "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\") # name is optional \n",
        "    sequence_output = transformer(input_word_ids)[0]\n",
        "\n",
        "    # to get the vector\n",
        "    cls_token = sequence_output[:, 0, :]\n",
        "    \n",
        "    # define output layer\n",
        "    out = Dense(1, activation='sigmoid')(cls_token)\n",
        "    \n",
        "    # initiate the model with inputs and outputs\n",
        "    model = Model(inputs=input_word_ids, outputs=out)\n",
        "    model.compile(Adam(lr=1e-5), loss='binary_crossentropy',metrics=[tf.keras.metrics.AUC()])\n",
        "    \n",
        "    return model"
      ],
      "metadata": {
        "id": "bCQ-UVhyww1q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocessing**"
      ],
      "metadata": {
        "id": "YTzCVIXHw6EX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configuration**"
      ],
      "metadata": {
        "id": "YXmjDbqSw8ts"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
        "strategy = tf.distribute.get_strategy()\n",
        "\n",
        "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aJStDVKNxN4F",
        "outputId": "54cde3d5-a442-4c88-cb0f-e47048fedb0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "REPLICAS:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# input pipeline that delivers data for the next step before the current step has finished.\n",
        "# The tf.data API helps to build flexible and efficient input pipelines.\n",
        "# This document demonstrates how to use the tf.data \n",
        "# API to build highly performant TensorFlow input pipelines.\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "# Configuration\n",
        "EPOCHS = 2\n",
        "#BATCH_SIZE = 16 * strategy.num_replicas_in_sync\n",
        "BATCH_SIZE = 2\n",
        "MAX_LEN = 192\n",
        "MODEL = 'bert-base-multilingual-cased'"
      ],
      "metadata": {
        "id": "RKqVj0Q1w_Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Import Dataset**"
      ],
      "metadata": {
        "id": "1kFydzc9xTjO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.read_csv(\"/content/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\")\n",
        "\n",
        "valid = pd.read_csv('/content/jigsaw-multilingual-toxic-comment-classification/validation.csv')\n",
        "test = pd.read_csv('/content/jigsaw-multilingual-toxic-comment-classification/test.csv')\n",
        "sub = pd.read_csv('/content/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv')"
      ],
      "metadata": {
        "id": "aACZNbBXxV9i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenizer**"
      ],
      "metadata": {
        "id": "ZcoB9ltlxseW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#use the pre-trained model bert as a tokenizer \n",
        "#bert tokenizer has vocabulary for emoji. this is the reason we don't need to remove emoji from \n",
        "#datasets, for more details see the (EDA & data cleaning) notebook\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)"
      ],
      "metadata": {
        "id": "88zXBb17xt_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Encode Comments**"
      ],
      "metadata": {
        "id": "XUwqMtHAxw11"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#call the function regular encode on for all the 3 dataset to convert each words after the tokenizer\n",
        "#into a vector\n",
        "#x_train,x_test, and x_validation will have the comment text column only,(in test called \"content\")\n",
        "x_train = regular_encode(train.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
        "x_valid = regular_encode(valid.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
        "x_test = regular_encode(test.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
        "\n",
        "#y_train,y_valid will have te target column \"toxic\"\n",
        "y_train = train.toxic.values\n",
        "y_valid = valid.toxic.values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SQyrdtZ0UoV",
        "outputId": "4e323771-d547-42bc-f263-abd7f97facaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2277: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare TensorFlow dataset for modeling**"
      ],
      "metadata": {
        "id": "802XXA3G3LXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a source dataset from your input data.\n",
        "# Apply dataset transformations to preprocess the data.\n",
        "# Iterate over the dataset and process the elements.\n",
        "train_dataset = (\n",
        "    tf.data.Dataset # create dataset\n",
        "    .from_tensor_slices((x_train, y_train)) # Once you have a dataset, you can apply transformations \n",
        "    .repeat()\n",
        "    .shuffle(2048)\n",
        "    .batch(BATCH_SIZE)# Combines consecutive elements of this dataset into batches.\n",
        "    .prefetch(AUTO) #This allows later elements to be prepared while the current element is being processed.\n",
        ")\n",
        "\n",
        "valid_dataset = (\n",
        "    tf.data.Dataset # create dataset\n",
        "    .from_tensor_slices((x_valid, y_valid)) # Once you have a dataset, you can apply transformations \n",
        "    .batch(BATCH_SIZE) #Combines consecutive elements of this dataset into batches.\n",
        "    .cache()\n",
        "    .prefetch(AUTO)#This allows later elements to be prepared while the current element is being processed.\n",
        ")\n",
        "\n",
        "test_dataset = (\n",
        "    tf.data.Dataset# create dataset\n",
        "    .from_tensor_slices(x_test) # Once you have a dataset, you can apply transformations \n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ],
      "metadata": {
        "id": "mtIYGN7V3MMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the model**"
      ],
      "metadata": {
        "id": "cBgcc-AG3SyP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Build the model**"
      ],
      "metadata": {
        "id": "7R2J6Vdd3WSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "# in the TPU\n",
        "with strategy.scope():\n",
        "    #take the encoder results of bert from transformers and use it as an input in the NN model\n",
        "    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n",
        "    model = build_model(transformer_layer, max_len=MAX_LEN)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2_vVhIN23XGt",
        "outputId": "01a7b3bd-7a39-43d2-c4a8-9939d47291cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-multilingual-cased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-multilingual-cased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_word_ids (InputLayer)  [(None, 192)]            0         \n",
            "                                                                 \n",
            " tf_bert_model (TFBertModel)  TFBaseModelOutputWithPoo  177853440\n",
            "                             lingAndCrossAttentions(l            \n",
            "                             ast_hidden_state=(None,             \n",
            "                             192, 768),                          \n",
            "                              pooler_output=(None, 76            \n",
            "                             8),                                 \n",
            "                              past_key_values=None, h            \n",
            "                             idden_states=None, atten            \n",
            "                             tions=None, cross_attent            \n",
            "                             ions=None)                          \n",
            "                                                                 \n",
            " tf.__operators__.getitem (S  (None, 768)              0         \n",
            " licingOpLambda)                                                 \n",
            "                                                                 \n",
            " dense (Dense)               (None, 1)                 769       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 177,854,209\n",
            "Trainable params: 177,854,209\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "CPU times: user 15.6 s, sys: 1.27 s, total: 16.8 s\n",
            "Wall time: 14.4 s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
            "  super(Adam, self).__init__(name, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training The Model, Tuning Hyper-Parameters**"
      ],
      "metadata": {
        "id": "bw0TQj5j3aEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#train the model\n",
        "# training the data and tune our model with the results of the metrics we get from the validation dataset\n",
        "n_steps = x_train.shape[0] // BATCH_SIZE\n",
        "train_history = model.fit(train_dataset, steps_per_epoch=n_steps, validation_data=valid_dataset,\n",
        "                epochs=EPOCHS)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Icp3pYrp3bEH",
        "outputId": "7795e731-9139-41be-94f4-efed4d59f93f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
            "   260/111774 [..............................] - ETA: 187:42:00 - loss: 0.3707 - auc: 0.5813"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing The Model**"
      ],
      "metadata": {
        "id": "6mJXYUSi3d59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#test the model on validation\n",
        "n_steps = x_valid.shape[0] // BATCH_SIZE\n",
        "train_history_2 = model.fit(valid_dataset.repeat(), steps_per_epoch=n_steps,epochs=EPOCHS*2)"
      ],
      "metadata": {
        "id": "CThwlDoz3epR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict and store the result**"
      ],
      "metadata": {
        "id": "u-MjEAJ63hRb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sub['toxic'] = model.predict(test_dataset, verbose=1)\n",
        "sub.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "l_a79Sce3iGO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}