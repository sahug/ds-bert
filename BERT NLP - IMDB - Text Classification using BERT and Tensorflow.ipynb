{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT NLP - IMDB - Text Classification using BERT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMkklXZujpQMBz2r9+joGep",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "76bd84000de04cbeab5048631049ac9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fd6053e9277e46c29b47b93b1188ebd8",
              "IPY_MODEL_c2c3466cecb54983a9eeb08fab05a43d",
              "IPY_MODEL_129e2ab2b57f495aa546a7b388b79715"
            ],
            "layout": "IPY_MODEL_46639306326a40b593a0ba065af69cdf"
          }
        },
        "fd6053e9277e46c29b47b93b1188ebd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c09cee77951d4433b0b7e6e7c1a78e84",
            "placeholder": "​",
            "style": "IPY_MODEL_f7dfb062cc82420da40db8dc0c1ab0c5",
            "value": "100%"
          }
        },
        "c2c3466cecb54983a9eeb08fab05a43d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8421c850821d4897ab0dc0a4e0624ed3",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce761d1744ec486d889d55426a28e08f",
            "value": 3
          }
        },
        "129e2ab2b57f495aa546a7b388b79715": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dd72f890644e4abd9ab15b3e5450cbfa",
            "placeholder": "​",
            "style": "IPY_MODEL_4c8f9dfe83104444be24ce126383cb88",
            "value": " 3/3 [00:00&lt;00:00,  4.83it/s]"
          }
        },
        "46639306326a40b593a0ba065af69cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c09cee77951d4433b0b7e6e7c1a78e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7dfb062cc82420da40db8dc0c1ab0c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8421c850821d4897ab0dc0a4e0624ed3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce761d1744ec486d889d55426a28e08f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dd72f890644e4abd9ab15b3e5450cbfa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c8f9dfe83104444be24ce126383cb88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahug/ds-bert/blob/main/BERT%20NLP%20-%20IMDB%20-%20Text%20Classification%20using%20BERT%20and%20Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**BERT NLP - IMDB - Text Classification using BERT**"
      ],
      "metadata": {
        "id": "zpWg9GJ-mPKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we will see how to fine-tune **DistilBERT** on the IMDb dataset to determine whether a movie review is positive or negative."
      ],
      "metadata": {
        "id": "I5ARwoGQmTfc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load Dataset**"
      ],
      "metadata": {
        "id": "rRBhHuQ3mVNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "Oecdwn9yaMN0"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qq datasets"
      ],
      "metadata": {
        "id": "EEQJ3pdzmgyF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "76bd84000de04cbeab5048631049ac9e",
            "fd6053e9277e46c29b47b93b1188ebd8",
            "c2c3466cecb54983a9eeb08fab05a43d",
            "129e2ab2b57f495aa546a7b388b79715",
            "46639306326a40b593a0ba065af69cdf",
            "c09cee77951d4433b0b7e6e7c1a78e84",
            "f7dfb062cc82420da40db8dc0c1ab0c5",
            "8421c850821d4897ab0dc0a4e0624ed3",
            "ce761d1744ec486d889d55426a28e08f",
            "dd72f890644e4abd9ab15b3e5450cbfa",
            "4c8f9dfe83104444be24ce126383cb88"
          ]
        },
        "id": "XrBkikSrmKJw",
        "outputId": "3c223694-0da5-48f8-d677-cec3e7edf0a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset imdb (/root/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "76bd84000de04cbeab5048631049ac9e"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "imdb = load_dataset(\"imdb\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L3SnC_fqYicx",
        "outputId": "5b2eb865-b14b-4bcd-a41b-ce3631a3e8c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 25000\n",
              "    })\n",
              "    unsupervised: Dataset({\n",
              "        features: ['text', 'label'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb[\"train\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRStQDUImxX_",
        "outputId": "804fd6b0-52e2-4673-fbf0-154eb9fd20c9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'label': 0,\n",
              " 'text': 'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'}"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb[\"train\"][\"text\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "xYE2X-yLZl57",
        "outputId": "b66a1ffc-bdb7-4f18-9a7e-829bd166a90f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb[\"train\"][\"label\"][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRm_kJObZytH",
        "outputId": "46bae90f-953d-4586-8e3a-e3ac24297b66"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.unique(imdb[\"train\"][\"label\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jGM6T6QXZ123",
        "outputId": "1bd521d1-055d-47a6-eea4-598caaa7f416"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Look at Dataset**"
      ],
      "metadata": {
        "id": "bgLFRo-lTXSF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import ClassLabel, Sequence\n",
        "import random\n",
        "import pandas as pd\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "def show_random_elements(dataset, num_examples=10):\n",
        "    assert num_examples <= len(\n",
        "        dataset\n",
        "    ), \"Can't pick more elements than there are in the dataset.\"\n",
        "    picks = []\n",
        "    for _ in range(num_examples):\n",
        "        pick = random.randint(0, len(dataset) - 1)\n",
        "        while pick in picks:\n",
        "            pick = random.randint(0, len(dataset) - 1)\n",
        "        picks.append(pick)\n",
        "\n",
        "    df = pd.DataFrame(dataset[picks])\n",
        "    for column, typ in dataset.features.items():\n",
        "        if isinstance(typ, ClassLabel):\n",
        "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
        "        elif isinstance(typ, Sequence) and isinstance(typ.feature, ClassLabel):\n",
        "            df[column] = df[column].transform(\n",
        "                lambda x: [typ.feature.names[i] for i in x]\n",
        "            )\n",
        "    display(HTML(df.to_html()))"
      ],
      "metadata": {
        "id": "VxAqg5wFTZFw"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_random_elements(imdb[\"train\"])"
      ],
      "metadata": {
        "id": "4HEg5DxqTabL",
        "outputId": "f8deb551-5607-429c-f465-dc8df7a33676",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>The film attempts to be a mockumentary--shot in the documentary form but with many obviously scripted parts--but fails in not providing the audience with any characters with which to create the illusion of the mockumentary. Also, the film purports to be about finding real love in Los Angeles, but is nothing more than an uninteresting look at an amateur filmmaker trying to make his first \"big movie.\"</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Emily Watson's Natalia is absolutely the most loving and romantic lead character I have ever seen on a screen. She is the queen of this film beyond all doubt. Or, is she transmuted to the king? The internecine weaving of the chess games and the families' struggles for control, power, and victory is stunning. Just as the chess masters in the film do, the director is playing many simultaneous games with our mind at once, but all weave into either major or minor patterns. The period, the costumes, and imagery of early 20th century Italy's lake district is captured magnificently. Not a single square of space is wasted.&lt;br /&gt;&lt;br /&gt;So many brilliant scenes abound, I cannot recount them all. I recommend budgeting enough time to watch this movie twice, possibly a week apart, because you can't possibly capture all the poetry within a 64-square yet multi-dimensional framework in one setting. &lt;br /&gt;&lt;br /&gt;I did not read Nabakov's book, but to try an analogy of my own, what I am reading reminds of me of another romantically triumphant poetry-as-game movie, Barry Levinson's The Natural. It totally jettisoned the downbeat ending of Bernard Malamud's fatalistic book in favor of a romantic impressionism that was uniquely American. Well, the director did that one better by seamlessly meshing Russian and Italian morals and mores as a backdrop to enlightenment. The true story here is that games are zero-sum; there is a winner and a loser, unless both contestants draw. But, in life, and especially in the context of our immortal souls, we are only limited by those constraints and life's conventions to the extent we let others break our spirit. &lt;br /&gt;&lt;br /&gt;Pure love, as personified by Emily Watson's Natalia, can transcend and allow all of us to be enhanced by its gifts simultaneously. Only the barriers erected by our fears can cut us off from it.&lt;br /&gt;&lt;br /&gt;This is a magnificent movie (10/10).</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>On the day of the California Presidential Primary, between midnight and 1:00 AM, the spy Victor Rovner sends a message from Kuala Lumpur to USA. Meanwhile, in Los Angeles, the Federal Agent Jack Bauer has returned to his family and is having trouble at home with his teenage daughter Kimberly, who blames her mother Teri for putting Jack out of the house. Teri and Jack decide to have a serious conversation with Kim, and they discover that the girl has run away home. While trying to solve his domestic problem, Jack is called to his Counter Terrorist Unit by his colleague Nina Myers for a meeting with their chief Richard Walsh, who discloses a menace against the life of Senator David Palmer, who is running for president, and they need to find the shooter. Later, Walsh has a private conversation with Jack and tells that there is a conspiracy in the agency against David Palmer, and assigns Jack to find the conspirators. When an airplane explodes over the Mojave Desert, Jack has one additional issue to worry about.&lt;br /&gt;&lt;br /&gt;The first episode of \"24\" is a promising beginning of a successful series, introducing Jack Bauer. This is the first time that I have watched this show and I confess that I liked what I have seen: a complex and dramatic story, with multiple and realistic characters. Kiefer Sutherland is perfect in the role of a family man and a reliable agent in charge of three difficult missions at the same time: find a killer to protect an important politician; find a traitor in his agency; and find his teenage daughter, who is getting in trouble, while trying to save his marriage. My vote is eight.&lt;br /&gt;&lt;br /&gt;Title (Brazil): \"12:00\"</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>I read several mixed reviews and several of them downright trashed the movie. I originally became interested in this project because it was being directed by Tony Scott and I have become very interested in his work after Man On Fire had such a profound impact on me. Before I start my review, let me first say this...it's wonderful to see that this movie could have been told in a boring and ordinary manner, yet the writers and Scott chose a different approach.&lt;br /&gt;&lt;br /&gt;Plot:&lt;br /&gt;&lt;br /&gt;Simply stated, it's not boring. Most Hollywood movies give 'tried and true' plots that they know will connect with people, often ensuring the audiences acceptance of the film and creating a higher probability of profit. This plot was one of the more interesting ones I had seen in a while. Just for reference, I recently watched 'The Weather Man' and 'Lord of War' and while I will say that these movies are excellent, and I enjoyed them both tremendously, both the plots in these movies are boring and they are told exactly how you would expect them to be told. They don't take any chances whatsoever, and they are extremely predictable after you've watched a fair amount of American films. Domino's plot is both interesting and told in a manner that keeps you thinking, \"oh man, they're screwed now\". And I feel that has been lacking in a lot of recent films. It has a lot of depth to it, in my opinion, and gives you plenty of things to question while watching it. Overall, this is what kept me so interested in the movie.&lt;br /&gt;&lt;br /&gt;Characters:&lt;br /&gt;&lt;br /&gt;I felt that the characters were accurate. Knightley did a wonderful job of portraying a beautiful woman, who was anything but on the inside and wanted to be viewed as what she was. It was obvious that she wanted to prove herself and she took whatever means she had to accomplish that.&lt;br /&gt;&lt;br /&gt;Choco was also very believable, his use of Spanish in inappropriate situations, his reactions to Domino's lack of affection, as well as his jealousy issues within the team...they all rang true to me, which made me feel that his character was that much more realistic.&lt;br /&gt;&lt;br /&gt;Rourke's character was the least interesting to me, but it still rang true to me. He seemed like an ordinary guy, trying to make ends meet. I hope that's what the filmmakers were trying to accomplish with him because that's what I got out of it. He did a very good job of showing Ed in an Average Joe kind of way that has made his mistakes, yet is still trying to live.&lt;br /&gt;&lt;br /&gt;Claremont/Ladies: I believe that they provided much needed 'heart' to the story. They weren't just people who are out getting money to buy a Bentley, these were real people who had a real problem and they sought others mean to accomplish that goal. You could empathize with them because, to them, this child's illness was a problem with no other solution. These characters were supposed to show real people who are less fortunate who got into this mess because they needed help.&lt;br /&gt;&lt;br /&gt;The mobsters: They made the story seem sinister in a way that only the mob can. And I really liked that part. They also padded the story with small intricacies that made the plot that much more interesting.&lt;br /&gt;&lt;br /&gt;Christopher Walken/90210 guys:&lt;br /&gt;&lt;br /&gt;They provided the comic relief in an otherwise very serious movie. From Walken's awkward statements to the ceaseless references to the 90210 guys being has-beens. Their involvement in the movie only made it that much more enjoyable.&lt;br /&gt;&lt;br /&gt;Cinematography....yes....the cinematography. This is where this movie seems to have lost a lot of potential fans. But in my opinion I thought it was genius, the use of the camera to translate the mood of the current situation was extremely effective in my opinion. I considered it a method that was properly realized but could always use improvement, just like anything else. I both applaud and congratulate Scott, the editor, the cinematographer and the director of photography on taking some real chances with this movie. Not only did they go far and above with its presentation, they went that much further. The use of colors, both extremely light and extremely dark provided the 'look' of the film with a sinister and grungy look that accurately depicts the life of the mob, bounty hunters and the less fortunate in a manner that show that their life isn't as peachy or 'clean' as everyone else. If you notice, in times of less stress or conflict, there were very few camera tricks if any at all. This shows that Scott and his crew were trying to achieve something with this look and weren't just doing it for the heck of it. I realize that most people who watched this movie weren't expecting it and it cause many of them to be turned off to this film but I think it was great that Scott took this approach. Hollywood films have grown predictable and bland. Most of them are shot in the same manner with the same twists and turns. And I'm glad that Scott tried to make something different.&lt;br /&gt;&lt;br /&gt;Granted, this movie isn't for everyone, but to say it's trash and has nothing to offer is completely missing the point. I thoroughly enjoyed this film and I'm glad that I spent the money for it. I would recommend this to all, but I'm sure it will only hit a chord with few. I must agree with an earlier poster when he said that many of those who refuse to see outside the 'sphere of MTV' won't appreciate this movie, but I think many people will. We should all try to enjoy it for the fact that Scott and co. took some chances and tried to deliver something that was different and unique. And with that in mind, I think he succeeded tremendously.</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Without question, the worst ELVIS film ever made. The movie portrays all Indians as drunk, stupid, and lazy. Watch ELVIS's skin change color throughout the film.</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>I caught Evening in the cinema with a lady friend. Evening is a chick flick with no apologies for being such, but I can say with some relief that it's not so infused with estrogen that it's painful for a red-blooded male to watch. Except for a single instance at the very end of the movie, I watched with interest and did not have to turn away or roll my eyes at any self-indulgent melodrama. Ladies, for their part, will absolutely love this movie.&lt;br /&gt;&lt;br /&gt;Ann Lord is elderly, bed-ridden and spending her last few days on Earth as comfortably as possible in her own home with her two grown daughters at her side. Discomfited by the memories of her past, Ann suddenly calls out a man's name her daughters have never heard before: Harris. While both of her daughters silently contemplate the significance of their mother's strong urge to recall and redress her ill-fated affair with this mysterious man at this of all times, Ann lapses back in her head to the fateful day she met Harris - and in doing so, lost the youthful optimism for the future that we all inevitably part ways with.&lt;br /&gt;&lt;br /&gt;Both Ann and her two daughters - one married with children, one a serial \"commitophobe\" - struggle with the central question of whether true love really exists, and perhaps more importantly, if true love can endure the test of time. Are we all one day fated to realize that love never lasts forever? Will we all realize that settling for the imperfect is the only realistic outcome? The subtle fact that the aged Ann is still wrestling with an answer to these questions on her deathbed is not lost on her two daughters.&lt;br /&gt;&lt;br /&gt;The cinematography for Evening is interesting - most of the film is spent in Ann's mind as she recalls the past, and for that reason I think the film was shot as if it was all deliberately overexposed, to give everyone an ethereal glow (and thus make it very obvious that all of this is not real, but occurred in the past). Claire Danes is beautiful (appearing to be really, really tall, though just 5' 5\" in reality), and is absolutely captivating in one climactic scene where her singing talents are finally put to the test.&lt;br /&gt;&lt;br /&gt;You can't really talk trash about the cast, which leads off with Claire Danes and doesn't let up from there: Vanessa Redgrave, Patrick Wilson, Meryl Streep and Glenn Close fill out the other major and minor roles in the film.&lt;br /&gt;&lt;br /&gt;I can't really say anything negative about this film at all, though Hugh Dancy's struggle to have his character emerge from utter one-dimensionality is in the end a total loss. Playing the spoiled, lovable drunk offspring of the obscenely rich who puts up a front of great bravado but is secretly scared stiff of never amounting to anything probably doesn't offer much in the way of character exploration - he had his orders and stuck to them.&lt;br /&gt;&lt;br /&gt;In the end, gentlemen, your lady friend will most certainly weep, and while you'll likely not feel nearly as affected, the evening will definitely not be a waste for the time spent watching Evening. Catch it in theatres or grab it as a rental to trade off for points for when you want to be accompanied to a viewing of Die Hard 4 or the upcoming Rambo flick. It'll be your little secret that this viewing didn't really cost you much at all.</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Typical 90's comedy, situational comedy similar to our modern day \"My Family\". Thatcher being the height of most political jokes, Bill (Belinda Lang) blames Thatcher for anything she can. \"Bloody Thatcher\" possibly shared with most of us. David the typical teenager, cutting up brains with bread knives, Jenny, the moody older teenage child, only interested in boys and more boys. Bill and Ben working as much as they can to keep their family afloat struggling within the economical climate of the early 90's. Granted the first two series were not as successful as the latter however, series 3 onwards is where it all kicks off with more laughs that i care to count. overall this show didn't get the best viewing times and they ought to have held on a bit longer. clearly they couldn't have carried on after Gary Olsen died but i think they should get rid of \"catherine Tate\" \"the office\" \"little Britain\" and bring back the classics!</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Movie industry is tricky business - because decisions have to be made and everyone involved has a private life, too. That's the very original thesis of this feeble attempt at making an 'insightful' film about film. And indeed, no better proof of the industry's trickiness than seeing Anouk Aimée and Maximilian Schell trapped in this inanity. The insight consists of talking heads rattle off bullshit like \"should I make a studio movie that pays a lot or should I make an indie item and stay true to my artistic self?\" \"Do the latter, please.\" Or: \"our relationship is not only professional, it's private as well. It's a rather complex situation to handle, isn't it?\" \"Yes, it is, my dear.\" Between the insipid dialogs one gets glimpses of palm trees, hotel lobbies and American movie posters (no sign of non-American film presence on the Croisette). Recurrent slumber sessions are inevitable, making the 100 minutes of the film feel like ages. Jenny Gabrielle is spectacularly unconvincing in justifying her own presence in the frame.</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Part Two picks up... not where the last film left off. As part of the quasi-conventionality of Steven Soderbergh's epic 4+ hour event, Che's two stories are told as classic \"Rise\" and \"Fall\" scenarios. In Part Two, Che Guevara, leaving his post as a bureaucrat in Cuba and after a failed attempt in the Congo (only in passing mentioned in the film), goes down to Bolivia to try and start up another through-the-jungle style revolution. Things don't go quite as well planned, at all, probably because of Che's then notorious stature as a Communist and revolutionary, and in part because of America's involvement on the side of the Bolivian Government, and, of course, that Castro wasn't really around as a back-up for Che.&lt;br /&gt;&lt;br /&gt;As it goes, the second part of Che is sadder, but in some ways wiser than the first part. Which makes sense, as Guevara has to endure low morale from his men, betrayals from those around him, constant mistakes by grunts and nearby peasants, and by ultimately the enclosing, larger military force. But what's sadder still is that Guevara, no matter what, won't give in. One may see this as an incredible strength or a fatal flaw- maybe both- but it's also clear how one starts to see Che, if not totally more fully rounded, then as something of a more sympathetic character. True, he did kill, and executed, and felt justified all the way. And yet it starts to work on the viewer in the sense of a primal level of pity; the sequence where Guevara's health worsens without medicine, leading up to the shocking stabbing of a horse, marks as one of the most memorable and satisfying of any film this year.&lt;br /&gt;&lt;br /&gt;Again, Soderbergh's command of narrative is strong, if, on occasion, slightly sluggish (understandable due to the big running time), and one or two scenes just feel totally odd (Matt Damon?), but these are minor liabilities. Going this time for the straight color camera approach, this is almost like a pure militia-style war picture, told with a great deal of care for the men in the group, as well as Guevara as the Lord-over this group, and how things dwindle down the final scene. And as always, Del-Toro is at the top of his game, in every scene, every beat knowing this guy so well- for better and for worse- that he comes about as close to embodiment as possible. Overall, the two parts of Che make up an impressive package: history as drama in compelling style, good for an audience even if they don't know Che or, better, if they don't think highly of him. It's that special. 8.5/10</td>\n",
              "      <td>pos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>For me this is a story that starts with some funny jokes regarding Franks fanatasies when he is travelling with a staircase and when he is sitting in business meetings... The problem is that when you have been watching this movie for an hour you will see the same fantasies/funny situations again and again and again. It is to predictable. It is more done as a TV story where you can go away and come back without missing anything.&lt;br /&gt;&lt;br /&gt;I like Felix Herngren as Frank but that is not enough even when it is a comedy it has to have more variations and some kind of message to it's audience....&lt;br /&gt;&lt;br /&gt;</td>\n",
              "      <td>neg</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train and Test Set**"
      ],
      "metadata": {
        "id": "wg5uZhj2aYSv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train, y_train = imdb[\"train\"][\"text\"], imdb[\"train\"][\"label\"]\n",
        "x_test, y_test = imdb[\"test\"][\"text\"], imdb[\"test\"][\"label\"]"
      ],
      "metadata": {
        "id": "ddFpUmPbaXjt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess**"
      ],
      "metadata": {
        "id": "6B54S5cxnQAr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qq tensorflow_hub\n",
        "%pip install -qq tensorflow_text"
      ],
      "metadata": {
        "id": "zWlgQSefWnn1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_hub as hub\n",
        "import tensorflow_text as text"
      ],
      "metadata": {
        "id": "cv4MDJXNXHTp"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessor = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
        "def pre_processor(example):\n",
        "  return preprocessor(example)\n",
        "\n",
        "pre_processor([imdb[\"train\"][\"text\"][0]])  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XxA-KtKfXb9G",
        "outputId": "133ccd8e-dad6-4ae0-9a1f-8fec14ce706c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_mask': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]],\n",
              "       dtype=int32)>,\n",
              " 'input_type_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "       dtype=int32)>,\n",
              " 'input_word_ids': <tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              " array([[  101,  1045, 12524,  1045,  2572,  8025,  1011,  3756,  2013,\n",
              "          2026,  2678,  3573,  2138,  1997,  2035,  1996,  6704,  2008,\n",
              "          5129,  2009,  2043,  2009,  2001,  2034,  2207,  1999,  3476,\n",
              "          1012,  1045,  2036,  2657,  2008,  2012,  2034,  2009,  2001,\n",
              "          8243,  2011,  1057,  1012,  1055,  1012,  8205,  2065,  2009,\n",
              "          2412,  2699,  2000,  4607,  2023,  2406,  1010,  3568,  2108,\n",
              "          1037,  5470,  1997,  3152,  2641,  1000,  6801,  1000,  1045,\n",
              "          2428,  2018,  2000,  2156,  2023,  2005,  2870,  1012,  1026,\n",
              "          7987,  1013,  1028,  1026,  7987,  1013,  1028,  1996,  5436,\n",
              "          2003,  8857,  2105,  1037,  2402,  4467,  3689,  3076,  2315,\n",
              "         14229,  2040,  4122,  2000,  4553,  2673,  2016,  2064,  2055,\n",
              "          2166,  1012,  1999,  3327,  2016,  4122,  2000,  3579,  2014,\n",
              "          3086,  2015,  2000,  2437,  2070,  4066,  1997,  4516,  2006,\n",
              "          2054,  1996,  2779, 25430, 14728,  2245,  2055,  3056,  2576,\n",
              "          3314,   102]], dtype=int32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
        "def encode_input(preprocessed_text):\n",
        "  return encoder(preprocessed_text)\n",
        "\n",
        "encoder(pre_processor([imdb[\"train\"][\"text\"][0]]))  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7ZpnWCNYOZK",
        "outputId": "f06df776-ca69-414a-b583-b5e297f91780"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'default': <tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
              " array([[-0.49452856, -0.49791604, -0.9572977 ,  0.34986803,  0.8465855 ,\n",
              "         -0.13578144, -0.5889829 ,  0.41979802, -0.83022875, -0.99984914,\n",
              "         -0.64828956,  0.9143599 ,  0.96066296,  0.46246734,  0.5033797 ,\n",
              "         -0.28232867,  0.43113717, -0.49907467,  0.3578105 ,  0.91972834,\n",
              "          0.39635044,  0.9999954 , -0.3181754 ,  0.44896606,  0.26785532,\n",
              "          0.9436459 , -0.55513763,  0.7681533 ,  0.7290908 ,  0.6581166 ,\n",
              "          0.2403225 ,  0.27039534, -0.9718417 , -0.16731699, -0.98095185,\n",
              "         -0.978594  ,  0.3959105 , -0.22986463,  0.03160511,  0.04872655,\n",
              "         -0.59069663,  0.31907785,  0.9999714 , -0.6494494 ,  0.5693388 ,\n",
              "         -0.21495499, -0.9999487 ,  0.27358246, -0.41509196,  0.88994277,\n",
              "          0.866684  ,  0.97687024,  0.24196966,  0.38024858,  0.39471295,\n",
              "         -0.6547417 , -0.15274712,  0.05646044, -0.27651197, -0.4769802 ,\n",
              "         -0.53807133,  0.3107337 , -0.7833923 , -0.65410084,  0.9052424 ,\n",
              "          0.8440127 , -0.14516944, -0.24017315,  0.03605577,  0.01976144,\n",
              "          0.3408388 ,  0.15069224, -0.5752722 , -0.7495797 ,  0.8131668 ,\n",
              "          0.45731685, -0.76896197,  1.        ,  0.30037853, -0.930795  ,\n",
              "          0.9630344 ,  0.819663  ,  0.66924685, -0.31495234,  0.3056318 ,\n",
              "         -0.9999995 ,  0.5396895 , -0.23265623, -0.95239586,  0.2806406 ,\n",
              "          0.50575984, -0.13129795,  0.9293615 ,  0.7169575 , -0.44585145,\n",
              "         -0.5397022 , -0.23132387, -0.9359872 , -0.13584934, -0.47069076,\n",
              "          0.14441599, -0.17785855, -0.33108976, -0.32582358,  0.3192142 ,\n",
              "         -0.49939853,  0.52119   ,  0.5892844 ,  0.22375095,  0.5110627 ,\n",
              "          0.6379116 , -0.44912115,  0.280306  , -0.7448627 ,  0.5100918 ,\n",
              "         -0.4442207 , -0.95135343, -0.61248153, -0.97039646,  0.44980487,\n",
              "         -0.22341649, -0.15893276,  0.6601814 , -0.6163227 ,  0.42665088,\n",
              "         -0.28175467, -0.9598344 , -1.        , -0.2826039 , -0.58691657,\n",
              "         -0.4789615 , -0.36955208, -0.91417027, -0.92138135,  0.36233053,\n",
              "          0.80769885,  0.17579444,  0.9998289 , -0.27243876,  0.8276758 ,\n",
              "         -0.09090931, -0.73511916,  0.70879406, -0.52165616,  0.62724954,\n",
              "         -0.83603173,  0.19127272,  0.23414633, -0.34936717,  0.3363312 ,\n",
              "         -0.7415785 , -0.2997296 , -0.8746284 , -0.5862444 , -0.45822495,\n",
              "          0.71681494, -0.81209874, -0.9261917 , -0.213536  , -0.15485343,\n",
              "         -0.08855458,  0.23752809,  0.44198215,  0.24073794, -0.35958084,\n",
              "          0.5171419 , -0.41211903,  0.19123109, -0.3952941 , -0.16380434,\n",
              "          0.18845834, -0.425096  , -0.9495775 , -0.9645378 , -0.33219352,\n",
              "          0.47459292,  0.9249224 ,  0.3303679 ,  0.2619356 ,  0.7685471 ,\n",
              "         -0.46388298,  0.56577957, -0.91084844,  0.9495422 ,  0.05534571,\n",
              "          0.3773883 , -0.9361992 ,  0.7545917 , -0.52661943,  0.09103332,\n",
              "         -0.104156  , -0.62443644, -0.4900307 , -0.03163054, -0.57941914,\n",
              "         -0.192626  , -0.86918265,  0.15590915, -0.23022904, -0.37318662,\n",
              "         -0.17663303,  0.8143341 , -0.11553032, -0.31320003,  0.6121321 ,\n",
              "          0.2455312 , -0.46740466, -0.22966565,  0.19440085,  0.10783081,\n",
              "          0.03114918,  0.9348749 , -0.8042481 ,  0.00505211, -0.7019624 ,\n",
              "         -0.94094336, -0.19638115, -0.6207106 , -0.23469092, -0.48364452,\n",
              "          0.6992982 , -0.90879107,  0.24897091,  0.26866302,  0.67304516,\n",
              "         -0.36226606,  0.18521453, -0.6786886 ,  0.49366117, -0.24866037,\n",
              "          0.9930445 ,  0.9798225 , -0.48645818, -0.91682005,  0.9409535 ,\n",
              "         -0.9877566 , -0.67610407, -0.535455  , -0.18607137,  0.04817577,\n",
              "         -0.6071178 ,  0.9557302 ,  0.87656695,  0.36866605, -0.57090867,\n",
              "         -0.90579957,  0.32946888, -0.57272935, -0.28438827, -0.1872985 ,\n",
              "          0.91759187,  0.74547815,  0.4572507 ,  0.61926514, -0.13788927,\n",
              "         -0.6410153 , -0.9988815 , -0.8720338 , -0.9946369 ,  0.20397118,\n",
              "         -0.9654712 ,  0.8843315 ,  0.19659182,  0.86300975, -0.43918055,\n",
              "         -0.13652837, -0.85977525, -0.5445266 ,  0.18071914,  0.3385084 ,\n",
              "         -0.73767906,  0.3356444 , -0.6357696 , -0.81600004,  0.09167261,\n",
              "         -0.09229537, -0.61280185, -0.0105057 , -0.64409816,  0.5784728 ,\n",
              "          0.5598801 ,  0.46388024, -0.9530843 ,  0.760778  ,  1.        ,\n",
              "          0.92081743,  0.56149167, -0.21953519, -0.9999467 , -0.9739119 ,\n",
              "          0.9999464 , -0.98891294, -1.        , -0.58018   , -0.445746  ,\n",
              "         -0.23420857, -1.        , -0.41096357, -0.02662418, -0.7857574 ,\n",
              "          0.660225  ,  0.9054854 , -0.30019435, -1.        ,  0.7945936 ,\n",
              "          0.6378658 , -0.7251465 ,  0.86100507, -0.45849225,  0.9022403 ,\n",
              "          0.60484856,  0.6004412 , -0.34065086,  0.5279204 , -0.97452897,\n",
              "          0.04178701, -0.8047813 , -0.9151828 ,  0.9993912 ,  0.06270785,\n",
              "         -0.3691827 , -0.5451419 ,  0.76187485, -0.09334386, -0.01728423,\n",
              "         -0.86982846, -0.32746968,  0.60999596,  0.5451634 ,  0.21847464,\n",
              "          0.34456533,  0.08408677,  0.25315398,  0.56743956, -0.5587267 ,\n",
              "          0.72399336, -0.8792934 ,  0.38348356,  0.4875311 ,  0.09647012,\n",
              "         -0.61884016, -0.9395884 ,  0.82255006, -0.47190642,  0.8080417 ,\n",
              "          1.        ,  0.90301627, -0.25105014,  0.4292362 ,  0.26704872,\n",
              "         -0.80268574,  1.        ,  0.7781706 , -0.94494194, -0.7098262 ,\n",
              "          0.73809946, -0.5878643 , -0.6069221 ,  0.9974328 , -0.16875494,\n",
              "         -0.8127117 , -0.58158255,  0.96850955, -0.95928085,  0.99898076,\n",
              "         -0.27043626, -0.89689803,  0.88469696,  0.7862594 , -0.43330204,\n",
              "         -0.5429266 ,  0.13275345, -0.3971729 ,  0.3653949 ,  0.09848042,\n",
              "          0.31682095,  0.2842838 , -0.04622179,  0.5419684 ,  0.85881066,\n",
              "         -0.7228363 ,  0.11067764, -0.6738337 , -0.17521742,  0.9704278 ,\n",
              "          0.3150708 , -0.14899227, -0.17454429, -0.23535831, -0.9603177 ,\n",
              "         -0.8723728 ,  0.64710945,  1.        , -0.40039077,  0.90888274,\n",
              "         -0.17042686, -0.10457391,  0.19543518,  0.5989221 ,  0.54226226,\n",
              "         -0.28130358, -0.57603645,  0.88086414, -0.07230149, -0.9812863 ,\n",
              "         -0.2860058 ,  0.156354  ,  0.03415044,  0.9985305 ,  0.3657011 ,\n",
              "          0.24485897,  0.7208754 ,  0.9806034 , -0.04402947, -0.41913503,\n",
              "          0.85500884,  0.94053596, -0.11423676,  0.72674656, -0.37384194,\n",
              "         -0.8901643 , -0.27666783, -0.50569063, -0.10371501, -0.87910396,\n",
              "          0.05666648, -0.83525723,  0.8187498 ,  0.9452383 ,  0.41212845,\n",
              "          0.22925541,  0.69145906,  1.        , -0.9924252 , -0.10386902,\n",
              "          0.98271006, -0.6451713 , -0.99996185, -0.13669991, -0.39914992,\n",
              "         -0.07536744, -0.9032237 , -0.28513816,  0.40398705, -0.88335186,\n",
              "          0.76560396,  0.8050262 ,  0.27346796, -0.92556363, -0.7775172 ,\n",
              "         -0.48574412,  0.17505738, -0.9890269 ,  0.03255482, -0.43051955,\n",
              "          0.5635246 , -0.27809253, -0.7222821 , -0.1433385 , -0.51501447,\n",
              "          0.413513  , -0.34591255,  0.74007046,  0.83721304,  0.90328634,\n",
              "         -0.95163494, -0.37101683, -0.18182601, -0.01457124,  0.0050352 ,\n",
              "         -0.29225478, -0.9265773 , -0.20132715,  1.        , -0.70205986,\n",
              "          0.8891981 , -0.05143772,  0.02306642, -0.23757096,  0.39318123,\n",
              "          0.96696067,  0.28630945, -0.60962266, -0.9197117 ,  0.9899376 ,\n",
              "         -0.24296822,  0.45774144,  0.8470791 ,  0.70078415,  0.4569391 ,\n",
              "          0.88795465,  0.00880828,  0.07944906,  0.01626093, -0.00700511,\n",
              "          0.08753929, -0.33627212, -0.07607547, -0.2267272 , -0.40767252,\n",
              "          0.94458365,  1.        ,  0.27533403,  0.6663032 , -0.9571236 ,\n",
              "         -0.8950675 , -0.13908109,  0.99999964,  0.80727005, -0.4536726 ,\n",
              "          0.6158869 ,  0.45567474, -0.26442665, -0.63722515, -0.26581407,\n",
              "         -0.1136687 ,  0.19765462, -0.06270554,  0.8816491 , -0.4825982 ,\n",
              "         -0.94406766, -0.114244  ,  0.26216528, -0.89238757,  0.9999741 ,\n",
              "         -0.63709074, -0.37521243, -0.20604001, -0.5347701 , -0.99847245,\n",
              "         -0.09577235, -0.9360463 , -0.31408465,  0.3142473 ,  0.80385023,\n",
              "          0.24578343, -0.713879  , -0.6425698 ,  0.9029424 ,  0.80125   ,\n",
              "         -0.9367304 , -0.8754765 ,  0.83802634, -0.8212577 ,  0.58915895,\n",
              "          0.9999963 ,  0.44084373,  0.35076287,  0.04799874, -0.04954723,\n",
              "          0.44917414, -0.5022856 ,  0.14115196, -0.848633  , -0.3454155 ,\n",
              "         -0.13195334,  0.3567666 , -0.10423731, -0.8194891 ,  0.03482563,\n",
              "          0.2927972 , -0.69043136, -0.62214077, -0.10060075,  0.34375888,\n",
              "          0.36988235, -0.15458733, -0.02677251,  0.16355062, -0.09477007,\n",
              "         -0.52255464, -0.44940224, -0.5417263 , -0.99999785,  0.2495743 ,\n",
              "         -1.        ,  0.7653906 ,  0.07182248, -0.22594538,  0.6256077 ,\n",
              "          0.9404093 ,  0.8591172 , -0.03711683, -0.92479396,  0.27553406,\n",
              "          0.48160434, -0.20638877,  0.00495878,  0.01346661,  0.38902885,\n",
              "         -0.04696539,  0.21953826, -0.61157197,  0.5290647 , -0.3064245 ,\n",
              "          1.        ,  0.1300141 , -0.24245128,  0.19730219,  0.24849464,\n",
              "         -0.27236056,  1.        ,  0.6777565 , -0.87526125,  0.27615795,\n",
              "         -0.6467428 , -0.33979115,  0.5140098 ,  0.06147739, -0.61682636,\n",
              "         -0.95310616,  0.25602785, -0.7147202 , -0.760799  ,  0.55526286,\n",
              "         -0.20624459, -0.17529424,  0.09136673,  0.94130385,  0.9462437 ,\n",
              "          0.6693023 , -0.12293503, -0.98045826, -0.687761  ,  0.894559  ,\n",
              "          0.42930803, -0.793383  ,  0.14416592,  1.        ,  0.3920436 ,\n",
              "         -0.5368446 , -0.05317557, -0.5592973 , -0.17402047, -0.5082954 ,\n",
              "          0.28421506,  0.07359429,  0.8909156 , -0.27084687,  0.76552284,\n",
              "         -0.9422423 ,  0.01002539, -0.4031697 , -0.66745186,  0.35788417,\n",
              "         -0.6312694 , -0.9536313 , -0.9562158 ,  0.7520106 , -0.30926406,\n",
              "         -0.17824668,  0.4677689 ,  0.14984445,  0.43025202,  0.33533528,\n",
              "         -1.        ,  0.9012649 ,  0.23027599,  0.88353693,  0.7743337 ,\n",
              "          0.74394566,  0.64666504,  0.31961572, -0.9050499 ,  0.27758104,\n",
              "         -0.13718791, -0.24743259,  0.03917703,  0.50748694,  0.5344401 ,\n",
              "          0.22606137, -0.5528945 , -0.8016556 , -0.7875179 , -0.9962027 ,\n",
              "         -0.9640058 ,  0.4584523 , -0.66996527,  0.6976017 ,  0.8547224 ,\n",
              "         -0.22146851, -0.05323618, -0.3828919 , -0.92491436, -0.83747655,\n",
              "          0.54143363, -0.4151773 , -0.05346537,  0.41605797,  0.51858836,\n",
              "          0.06519934,  0.9183956 , -0.92177236, -0.32098645, -0.8756186 ,\n",
              "          0.5004361 ,  0.9801905 , -0.91063964,  0.14475182,  0.6583333 ,\n",
              "         -0.18443774,  0.36420593, -0.4240014 ,  0.1619866 ,  0.8620015 ,\n",
              "         -0.33198404,  0.1990656 , -0.4081088 , -0.07254337, -0.33869258,\n",
              "         -0.30839103, -0.64051986, -0.61156255,  0.7253657 , -0.2888664 ,\n",
              "          0.3900304 ,  0.9067333 , -0.03222425, -0.08755511, -0.07373793,\n",
              "         -0.65570635, -0.7272714 , -0.5967959 ,  0.07235501,  0.26347804,\n",
              "          0.5225863 , -0.29539654,  0.992342  ,  0.3983035 , -0.2544526 ,\n",
              "         -0.28120327, -0.45584625,  0.47195113, -0.8864588 , -0.50867313,\n",
              "         -0.30237466,  0.43593967,  0.23287667,  0.9999959 , -0.8411575 ,\n",
              "         -0.8716144 , -0.63739634, -0.24933548,  0.58289844, -0.1031876 ,\n",
              "         -1.        ,  0.22786607, -0.77909964,  0.8523781 , -0.7560842 ,\n",
              "          0.9448525 , -0.66515756, -0.36977026, -0.36001983,  0.88523424,\n",
              "          0.89223564, -0.366456  , -0.27651444,  0.70607847, -0.5325352 ,\n",
              "          0.9648017 ,  0.19735828, -0.13097401,  0.21240462,  0.7766575 ,\n",
              "         -0.9021658 , -0.48400053,  0.52324843]], dtype=float32)>,\n",
              " 'encoder_outputs': [<tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.02157287,  0.02454674, -0.23579627, ...,  0.21620415,\n",
              "           -0.03810631,  0.00287605],\n",
              "          [ 0.37743717,  0.3516994 , -0.30859452, ...,  0.7662563 ,\n",
              "            1.0308214 ,  0.24931544],\n",
              "          [-1.624133  , -0.82388335, -0.7251606 , ...,  0.39058238,\n",
              "            0.3865813 ,  0.81231123],\n",
              "          ...,\n",
              "          [-0.2461331 ,  0.6463891 ,  0.44864523, ..., -0.45399573,\n",
              "           -0.3489546 , -0.34182274],\n",
              "          [-1.4276469 ,  1.02377   ,  2.1305425 , ..., -0.14480048,\n",
              "           -0.40842482, -0.37170658],\n",
              "          [-0.0633731 ,  0.2574133 ,  0.22398448, ..., -0.00592242,\n",
              "            0.09640907, -0.39369524]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[-0.05290899, -0.19746265, -0.41062343, ...,  0.34556666,\n",
              "            0.15540734, -0.03720344],\n",
              "          [ 0.28935206,  0.15280172, -0.17668626, ...,  0.6331961 ,\n",
              "            0.93766266,  0.31146452],\n",
              "          [-1.2064773 , -1.0606856 , -0.54029876, ...,  0.37202623,\n",
              "            0.4790625 ,  0.24338894],\n",
              "          ...,\n",
              "          [-0.34928265,  0.8136606 ,  0.41588724, ..., -0.03806745,\n",
              "           -0.2889181 , -0.4111918 ],\n",
              "          [-0.92299783,  0.9233547 ,  1.7679037 , ...,  0.044759  ,\n",
              "           -0.24505675, -0.5372085 ],\n",
              "          [ 0.12469389,  0.02300367,  0.01516747, ..., -0.04061016,\n",
              "            0.1580104 , -0.7128955 ]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.03304805, -0.33662003, -0.24180707, ...,  0.2996758 ,\n",
              "            0.25331846,  0.03673234],\n",
              "          [ 0.30516592,  0.26332647,  0.03416043, ...,  0.41202512,\n",
              "            1.0396692 ,  0.35787317],\n",
              "          [-0.8674692 , -0.774152  , -0.34159392, ...,  0.00462754,\n",
              "            0.24779896,  0.36202264],\n",
              "          ...,\n",
              "          [-0.2127604 ,  0.6492671 ,  0.28097194, ..., -0.20257834,\n",
              "           -0.3259863 , -0.16518402],\n",
              "          [-0.66742873,  0.55237776,  1.4232227 , ...,  0.40313455,\n",
              "           -0.35528585, -0.68823504],\n",
              "          [-0.02131845, -0.08209501,  0.07003066, ...,  0.03216621,\n",
              "            0.05108055, -0.1476939 ]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.28944272, -0.73453003, -0.65570664, ...,  0.25447753,\n",
              "            0.23645489,  0.39594716],\n",
              "          [ 0.6463696 , -0.12045646, -0.63815594, ...,  0.5391103 ,\n",
              "            0.8126906 ,  0.5717903 ],\n",
              "          [-0.42752016, -0.90247273, -0.15973853, ..., -0.08489319,\n",
              "            0.21617293,  0.70195884],\n",
              "          ...,\n",
              "          [-0.07506715,  0.5523565 ,  0.61078626, ...,  0.18533997,\n",
              "           -0.5816471 ,  0.3658401 ],\n",
              "          [-0.36729693,  0.42395112,  0.96394217, ...,  0.9372702 ,\n",
              "           -0.7595536 , -0.4642778 ],\n",
              "          [ 0.00276151, -0.0486895 , -0.00885086, ...,  0.01233935,\n",
              "            0.03494243, -0.05467367]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.19944488, -0.9341455 , -0.55044883, ..., -0.07476664,\n",
              "            0.19069615,  0.5111912 ],\n",
              "          [ 0.9046634 , -0.25381964, -0.5818953 , ...,  0.4850065 ,\n",
              "            1.0236492 ,  0.6945227 ],\n",
              "          [-0.24008428, -0.6053609 , -0.54751563, ..., -0.03457375,\n",
              "            0.18650699,  0.79142004],\n",
              "          ...,\n",
              "          [-0.09100676,  0.36776853,  0.56024843, ...,  0.10132753,\n",
              "           -0.32129   ,  0.62833667],\n",
              "          [-0.5112093 , -0.2023352 ,  0.5722098 , ...,  0.99921465,\n",
              "           -0.63208765, -0.699623  ],\n",
              "          [-0.01118472, -0.0328205 ,  0.02533866, ..., -0.00524814,\n",
              "            0.00413203, -0.04512979]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.29990003, -1.3067366 , -0.6210294 , ..., -0.05192786,\n",
              "            0.39166355,  0.34250337],\n",
              "          [ 0.4246294 , -0.5662254 , -0.9458901 , ...,  0.13909361,\n",
              "            0.7788996 ,  0.40953204],\n",
              "          [-0.20012794, -0.37217063, -1.1831764 , ..., -0.32507873,\n",
              "            0.11992379,  0.9101696 ],\n",
              "          ...,\n",
              "          [ 0.01034607,  0.1196017 ,  0.7075085 , ...,  0.352365  ,\n",
              "           -0.46298215,  1.0067694 ],\n",
              "          [-0.49650672,  0.08256728,  0.59949213, ...,  0.7395518 ,\n",
              "           -0.69218963, -0.5634303 ],\n",
              "          [ 0.02886646, -0.03438197, -0.02131964, ..., -0.01894899,\n",
              "           -0.01024931, -0.03020475]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.31523022, -1.1628895 , -0.29079413, ..., -0.11056396,\n",
              "            0.22961657,  0.37558827],\n",
              "          [ 0.14578907, -0.4336442 , -0.6130982 , ...,  0.12232678,\n",
              "            0.8893812 ,  0.41577715],\n",
              "          [ 0.20321593,  0.15378484, -0.7671167 , ...,  0.13571778,\n",
              "            0.6210884 ,  1.0719639 ],\n",
              "          ...,\n",
              "          [ 0.007232  , -0.15948881,  0.60431445, ...,  0.05570034,\n",
              "           -0.4559719 ,  1.4729291 ],\n",
              "          [-0.42183056,  0.14978248,  0.32672083, ...,  0.7398286 ,\n",
              "           -0.19533499, -0.44311497],\n",
              "          [ 0.04866537,  0.0196248 , -0.02040474, ..., -0.04427862,\n",
              "            0.01056362, -0.02959514]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 0.38220066, -0.9878383 , -0.33453724, ..., -0.3440116 ,\n",
              "            0.2122592 ,  0.3148623 ],\n",
              "          [ 0.38410935, -0.6536184 , -0.7907777 , ...,  0.06298618,\n",
              "            0.5934244 ,  0.24806729],\n",
              "          [ 0.28541327,  0.24644029, -0.24994142, ...,  0.04265312,\n",
              "            0.77965546,  0.9644861 ],\n",
              "          ...,\n",
              "          [-0.28275415, -0.048629  ,  0.45078635, ...,  0.08672051,\n",
              "           -0.36185625,  0.9129159 ],\n",
              "          [-0.6788607 , -0.0558942 ,  0.10732361, ...,  0.6070429 ,\n",
              "           -0.16992687, -0.59365344],\n",
              "          [ 0.05334233,  0.03220662,  0.00400462, ..., -0.02226178,\n",
              "           -0.04272716, -0.05920484]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[ 3.2771850e-01, -6.3277423e-01, -1.1523955e-01, ...,\n",
              "           -6.4891595e-03,  3.9806074e-01,  2.8845111e-02],\n",
              "          [ 2.7816403e-01, -2.6278484e-01, -3.2368875e-01, ...,\n",
              "            2.0243061e-01,  4.1805607e-01, -1.9305372e-01],\n",
              "          [ 3.7270087e-01,  1.6084123e-01, -4.1641593e-02, ...,\n",
              "            2.3448284e-01,  5.6128877e-01,  7.7293324e-01],\n",
              "          ...,\n",
              "          [-2.9157996e-01, -4.2261752e-01,  3.0669218e-01, ...,\n",
              "           -3.3207667e-01, -8.5675728e-01,  8.8722146e-01],\n",
              "          [-8.3596075e-01, -1.9679207e-01,  5.5934694e-03, ...,\n",
              "           -1.0645737e-01, -4.8772487e-01, -5.0952148e-01],\n",
              "          [ 6.1332621e-04, -1.0660654e-02,  7.2744079e-03, ...,\n",
              "           -3.8654558e-02,  1.4529815e-02, -8.1656724e-02]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[-0.24816208, -0.67402554,  0.2181504 , ...,  0.12296968,\n",
              "            0.03825888, -0.03143629],\n",
              "          [ 0.69193614, -0.49439314, -0.2455369 , ..., -0.2340518 ,\n",
              "           -0.1933953 , -0.21274835],\n",
              "          [ 0.60410064, -0.13916534,  0.12259461, ...,  0.63537735,\n",
              "           -0.24980727,  0.4504054 ],\n",
              "          ...,\n",
              "          [-0.20909077, -0.5322157 ,  0.11240845, ..., -0.44509965,\n",
              "           -1.0017376 ,  0.66457784],\n",
              "          [-0.7373631 , -0.18511084, -0.43446508, ..., -0.27929354,\n",
              "           -0.8584175 , -0.7553869 ],\n",
              "          [-0.0494957 , -0.13160154,  0.09722381, ...,  0.36188063,\n",
              "            0.04810295, -0.14351283]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[-0.13005272, -0.65316474,  0.10821369, ..., -0.02012793,\n",
              "            0.5325192 ,  0.19532566],\n",
              "          [ 0.54514605, -0.61996293, -0.1677625 , ..., -0.3111193 ,\n",
              "           -0.25934651, -0.00464241],\n",
              "          [ 0.8183983 , -0.08599478,  0.22135174, ...,  0.6103374 ,\n",
              "           -0.5377734 ,  0.49608704],\n",
              "          ...,\n",
              "          [-0.4203497 , -0.20289582,  0.00350174, ..., -0.3911443 ,\n",
              "           -1.1249082 ,  0.5179315 ],\n",
              "          [-1.1821991 ,  0.14726497, -0.7378723 , ..., -0.67802   ,\n",
              "           -0.94131607, -0.5380174 ],\n",
              "          [-0.00513178, -0.00668822,  0.19685711, ...,  0.47825143,\n",
              "           -0.08537105, -0.17680681]]], dtype=float32)>,\n",
              "  <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              "  array([[[-5.5853043e-02, -1.9755675e-01,  6.4958468e-02, ...,\n",
              "           -2.1587220e-01,  5.9806556e-01,  4.3526551e-01],\n",
              "          [ 4.8271894e-02, -1.9900392e-01, -5.0419092e-01, ...,\n",
              "           -8.5937046e-02,  2.7296653e-01,  2.9258022e-01],\n",
              "          [ 8.3109325e-01,  2.0109113e-02, -1.1282250e-02, ...,\n",
              "            5.2618271e-01, -1.0882966e-02,  1.8892158e-04],\n",
              "          ...,\n",
              "          [-2.6907486e-01, -3.0361760e-01,  1.4330816e-01, ...,\n",
              "           -6.7106321e-02, -8.0915833e-01,  3.7342048e-01],\n",
              "          [-8.0237484e-01,  2.0807880e-01, -4.6244180e-01, ...,\n",
              "           -2.6407379e-01, -6.3195539e-01, -4.3557918e-01],\n",
              "          [-5.1775598e-03,  3.0852658e-01,  3.1194410e-01, ...,\n",
              "            4.1307107e-01, -2.5533882e-01, -1.1421091e-01]]], dtype=float32)>],\n",
              " 'pooled_output': <tf.Tensor: shape=(1, 768), dtype=float32, numpy=\n",
              " array([[-0.49452856, -0.49791604, -0.9572977 ,  0.34986803,  0.8465855 ,\n",
              "         -0.13578144, -0.5889829 ,  0.41979802, -0.83022875, -0.99984914,\n",
              "         -0.64828956,  0.9143599 ,  0.96066296,  0.46246734,  0.5033797 ,\n",
              "         -0.28232867,  0.43113717, -0.49907467,  0.3578105 ,  0.91972834,\n",
              "          0.39635044,  0.9999954 , -0.3181754 ,  0.44896606,  0.26785532,\n",
              "          0.9436459 , -0.55513763,  0.7681533 ,  0.7290908 ,  0.6581166 ,\n",
              "          0.2403225 ,  0.27039534, -0.9718417 , -0.16731699, -0.98095185,\n",
              "         -0.978594  ,  0.3959105 , -0.22986463,  0.03160511,  0.04872655,\n",
              "         -0.59069663,  0.31907785,  0.9999714 , -0.6494494 ,  0.5693388 ,\n",
              "         -0.21495499, -0.9999487 ,  0.27358246, -0.41509196,  0.88994277,\n",
              "          0.866684  ,  0.97687024,  0.24196966,  0.38024858,  0.39471295,\n",
              "         -0.6547417 , -0.15274712,  0.05646044, -0.27651197, -0.4769802 ,\n",
              "         -0.53807133,  0.3107337 , -0.7833923 , -0.65410084,  0.9052424 ,\n",
              "          0.8440127 , -0.14516944, -0.24017315,  0.03605577,  0.01976144,\n",
              "          0.3408388 ,  0.15069224, -0.5752722 , -0.7495797 ,  0.8131668 ,\n",
              "          0.45731685, -0.76896197,  1.        ,  0.30037853, -0.930795  ,\n",
              "          0.9630344 ,  0.819663  ,  0.66924685, -0.31495234,  0.3056318 ,\n",
              "         -0.9999995 ,  0.5396895 , -0.23265623, -0.95239586,  0.2806406 ,\n",
              "          0.50575984, -0.13129795,  0.9293615 ,  0.7169575 , -0.44585145,\n",
              "         -0.5397022 , -0.23132387, -0.9359872 , -0.13584934, -0.47069076,\n",
              "          0.14441599, -0.17785855, -0.33108976, -0.32582358,  0.3192142 ,\n",
              "         -0.49939853,  0.52119   ,  0.5892844 ,  0.22375095,  0.5110627 ,\n",
              "          0.6379116 , -0.44912115,  0.280306  , -0.7448627 ,  0.5100918 ,\n",
              "         -0.4442207 , -0.95135343, -0.61248153, -0.97039646,  0.44980487,\n",
              "         -0.22341649, -0.15893276,  0.6601814 , -0.6163227 ,  0.42665088,\n",
              "         -0.28175467, -0.9598344 , -1.        , -0.2826039 , -0.58691657,\n",
              "         -0.4789615 , -0.36955208, -0.91417027, -0.92138135,  0.36233053,\n",
              "          0.80769885,  0.17579444,  0.9998289 , -0.27243876,  0.8276758 ,\n",
              "         -0.09090931, -0.73511916,  0.70879406, -0.52165616,  0.62724954,\n",
              "         -0.83603173,  0.19127272,  0.23414633, -0.34936717,  0.3363312 ,\n",
              "         -0.7415785 , -0.2997296 , -0.8746284 , -0.5862444 , -0.45822495,\n",
              "          0.71681494, -0.81209874, -0.9261917 , -0.213536  , -0.15485343,\n",
              "         -0.08855458,  0.23752809,  0.44198215,  0.24073794, -0.35958084,\n",
              "          0.5171419 , -0.41211903,  0.19123109, -0.3952941 , -0.16380434,\n",
              "          0.18845834, -0.425096  , -0.9495775 , -0.9645378 , -0.33219352,\n",
              "          0.47459292,  0.9249224 ,  0.3303679 ,  0.2619356 ,  0.7685471 ,\n",
              "         -0.46388298,  0.56577957, -0.91084844,  0.9495422 ,  0.05534571,\n",
              "          0.3773883 , -0.9361992 ,  0.7545917 , -0.52661943,  0.09103332,\n",
              "         -0.104156  , -0.62443644, -0.4900307 , -0.03163054, -0.57941914,\n",
              "         -0.192626  , -0.86918265,  0.15590915, -0.23022904, -0.37318662,\n",
              "         -0.17663303,  0.8143341 , -0.11553032, -0.31320003,  0.6121321 ,\n",
              "          0.2455312 , -0.46740466, -0.22966565,  0.19440085,  0.10783081,\n",
              "          0.03114918,  0.9348749 , -0.8042481 ,  0.00505211, -0.7019624 ,\n",
              "         -0.94094336, -0.19638115, -0.6207106 , -0.23469092, -0.48364452,\n",
              "          0.6992982 , -0.90879107,  0.24897091,  0.26866302,  0.67304516,\n",
              "         -0.36226606,  0.18521453, -0.6786886 ,  0.49366117, -0.24866037,\n",
              "          0.9930445 ,  0.9798225 , -0.48645818, -0.91682005,  0.9409535 ,\n",
              "         -0.9877566 , -0.67610407, -0.535455  , -0.18607137,  0.04817577,\n",
              "         -0.6071178 ,  0.9557302 ,  0.87656695,  0.36866605, -0.57090867,\n",
              "         -0.90579957,  0.32946888, -0.57272935, -0.28438827, -0.1872985 ,\n",
              "          0.91759187,  0.74547815,  0.4572507 ,  0.61926514, -0.13788927,\n",
              "         -0.6410153 , -0.9988815 , -0.8720338 , -0.9946369 ,  0.20397118,\n",
              "         -0.9654712 ,  0.8843315 ,  0.19659182,  0.86300975, -0.43918055,\n",
              "         -0.13652837, -0.85977525, -0.5445266 ,  0.18071914,  0.3385084 ,\n",
              "         -0.73767906,  0.3356444 , -0.6357696 , -0.81600004,  0.09167261,\n",
              "         -0.09229537, -0.61280185, -0.0105057 , -0.64409816,  0.5784728 ,\n",
              "          0.5598801 ,  0.46388024, -0.9530843 ,  0.760778  ,  1.        ,\n",
              "          0.92081743,  0.56149167, -0.21953519, -0.9999467 , -0.9739119 ,\n",
              "          0.9999464 , -0.98891294, -1.        , -0.58018   , -0.445746  ,\n",
              "         -0.23420857, -1.        , -0.41096357, -0.02662418, -0.7857574 ,\n",
              "          0.660225  ,  0.9054854 , -0.30019435, -1.        ,  0.7945936 ,\n",
              "          0.6378658 , -0.7251465 ,  0.86100507, -0.45849225,  0.9022403 ,\n",
              "          0.60484856,  0.6004412 , -0.34065086,  0.5279204 , -0.97452897,\n",
              "          0.04178701, -0.8047813 , -0.9151828 ,  0.9993912 ,  0.06270785,\n",
              "         -0.3691827 , -0.5451419 ,  0.76187485, -0.09334386, -0.01728423,\n",
              "         -0.86982846, -0.32746968,  0.60999596,  0.5451634 ,  0.21847464,\n",
              "          0.34456533,  0.08408677,  0.25315398,  0.56743956, -0.5587267 ,\n",
              "          0.72399336, -0.8792934 ,  0.38348356,  0.4875311 ,  0.09647012,\n",
              "         -0.61884016, -0.9395884 ,  0.82255006, -0.47190642,  0.8080417 ,\n",
              "          1.        ,  0.90301627, -0.25105014,  0.4292362 ,  0.26704872,\n",
              "         -0.80268574,  1.        ,  0.7781706 , -0.94494194, -0.7098262 ,\n",
              "          0.73809946, -0.5878643 , -0.6069221 ,  0.9974328 , -0.16875494,\n",
              "         -0.8127117 , -0.58158255,  0.96850955, -0.95928085,  0.99898076,\n",
              "         -0.27043626, -0.89689803,  0.88469696,  0.7862594 , -0.43330204,\n",
              "         -0.5429266 ,  0.13275345, -0.3971729 ,  0.3653949 ,  0.09848042,\n",
              "          0.31682095,  0.2842838 , -0.04622179,  0.5419684 ,  0.85881066,\n",
              "         -0.7228363 ,  0.11067764, -0.6738337 , -0.17521742,  0.9704278 ,\n",
              "          0.3150708 , -0.14899227, -0.17454429, -0.23535831, -0.9603177 ,\n",
              "         -0.8723728 ,  0.64710945,  1.        , -0.40039077,  0.90888274,\n",
              "         -0.17042686, -0.10457391,  0.19543518,  0.5989221 ,  0.54226226,\n",
              "         -0.28130358, -0.57603645,  0.88086414, -0.07230149, -0.9812863 ,\n",
              "         -0.2860058 ,  0.156354  ,  0.03415044,  0.9985305 ,  0.3657011 ,\n",
              "          0.24485897,  0.7208754 ,  0.9806034 , -0.04402947, -0.41913503,\n",
              "          0.85500884,  0.94053596, -0.11423676,  0.72674656, -0.37384194,\n",
              "         -0.8901643 , -0.27666783, -0.50569063, -0.10371501, -0.87910396,\n",
              "          0.05666648, -0.83525723,  0.8187498 ,  0.9452383 ,  0.41212845,\n",
              "          0.22925541,  0.69145906,  1.        , -0.9924252 , -0.10386902,\n",
              "          0.98271006, -0.6451713 , -0.99996185, -0.13669991, -0.39914992,\n",
              "         -0.07536744, -0.9032237 , -0.28513816,  0.40398705, -0.88335186,\n",
              "          0.76560396,  0.8050262 ,  0.27346796, -0.92556363, -0.7775172 ,\n",
              "         -0.48574412,  0.17505738, -0.9890269 ,  0.03255482, -0.43051955,\n",
              "          0.5635246 , -0.27809253, -0.7222821 , -0.1433385 , -0.51501447,\n",
              "          0.413513  , -0.34591255,  0.74007046,  0.83721304,  0.90328634,\n",
              "         -0.95163494, -0.37101683, -0.18182601, -0.01457124,  0.0050352 ,\n",
              "         -0.29225478, -0.9265773 , -0.20132715,  1.        , -0.70205986,\n",
              "          0.8891981 , -0.05143772,  0.02306642, -0.23757096,  0.39318123,\n",
              "          0.96696067,  0.28630945, -0.60962266, -0.9197117 ,  0.9899376 ,\n",
              "         -0.24296822,  0.45774144,  0.8470791 ,  0.70078415,  0.4569391 ,\n",
              "          0.88795465,  0.00880828,  0.07944906,  0.01626093, -0.00700511,\n",
              "          0.08753929, -0.33627212, -0.07607547, -0.2267272 , -0.40767252,\n",
              "          0.94458365,  1.        ,  0.27533403,  0.6663032 , -0.9571236 ,\n",
              "         -0.8950675 , -0.13908109,  0.99999964,  0.80727005, -0.4536726 ,\n",
              "          0.6158869 ,  0.45567474, -0.26442665, -0.63722515, -0.26581407,\n",
              "         -0.1136687 ,  0.19765462, -0.06270554,  0.8816491 , -0.4825982 ,\n",
              "         -0.94406766, -0.114244  ,  0.26216528, -0.89238757,  0.9999741 ,\n",
              "         -0.63709074, -0.37521243, -0.20604001, -0.5347701 , -0.99847245,\n",
              "         -0.09577235, -0.9360463 , -0.31408465,  0.3142473 ,  0.80385023,\n",
              "          0.24578343, -0.713879  , -0.6425698 ,  0.9029424 ,  0.80125   ,\n",
              "         -0.9367304 , -0.8754765 ,  0.83802634, -0.8212577 ,  0.58915895,\n",
              "          0.9999963 ,  0.44084373,  0.35076287,  0.04799874, -0.04954723,\n",
              "          0.44917414, -0.5022856 ,  0.14115196, -0.848633  , -0.3454155 ,\n",
              "         -0.13195334,  0.3567666 , -0.10423731, -0.8194891 ,  0.03482563,\n",
              "          0.2927972 , -0.69043136, -0.62214077, -0.10060075,  0.34375888,\n",
              "          0.36988235, -0.15458733, -0.02677251,  0.16355062, -0.09477007,\n",
              "         -0.52255464, -0.44940224, -0.5417263 , -0.99999785,  0.2495743 ,\n",
              "         -1.        ,  0.7653906 ,  0.07182248, -0.22594538,  0.6256077 ,\n",
              "          0.9404093 ,  0.8591172 , -0.03711683, -0.92479396,  0.27553406,\n",
              "          0.48160434, -0.20638877,  0.00495878,  0.01346661,  0.38902885,\n",
              "         -0.04696539,  0.21953826, -0.61157197,  0.5290647 , -0.3064245 ,\n",
              "          1.        ,  0.1300141 , -0.24245128,  0.19730219,  0.24849464,\n",
              "         -0.27236056,  1.        ,  0.6777565 , -0.87526125,  0.27615795,\n",
              "         -0.6467428 , -0.33979115,  0.5140098 ,  0.06147739, -0.61682636,\n",
              "         -0.95310616,  0.25602785, -0.7147202 , -0.760799  ,  0.55526286,\n",
              "         -0.20624459, -0.17529424,  0.09136673,  0.94130385,  0.9462437 ,\n",
              "          0.6693023 , -0.12293503, -0.98045826, -0.687761  ,  0.894559  ,\n",
              "          0.42930803, -0.793383  ,  0.14416592,  1.        ,  0.3920436 ,\n",
              "         -0.5368446 , -0.05317557, -0.5592973 , -0.17402047, -0.5082954 ,\n",
              "          0.28421506,  0.07359429,  0.8909156 , -0.27084687,  0.76552284,\n",
              "         -0.9422423 ,  0.01002539, -0.4031697 , -0.66745186,  0.35788417,\n",
              "         -0.6312694 , -0.9536313 , -0.9562158 ,  0.7520106 , -0.30926406,\n",
              "         -0.17824668,  0.4677689 ,  0.14984445,  0.43025202,  0.33533528,\n",
              "         -1.        ,  0.9012649 ,  0.23027599,  0.88353693,  0.7743337 ,\n",
              "          0.74394566,  0.64666504,  0.31961572, -0.9050499 ,  0.27758104,\n",
              "         -0.13718791, -0.24743259,  0.03917703,  0.50748694,  0.5344401 ,\n",
              "          0.22606137, -0.5528945 , -0.8016556 , -0.7875179 , -0.9962027 ,\n",
              "         -0.9640058 ,  0.4584523 , -0.66996527,  0.6976017 ,  0.8547224 ,\n",
              "         -0.22146851, -0.05323618, -0.3828919 , -0.92491436, -0.83747655,\n",
              "          0.54143363, -0.4151773 , -0.05346537,  0.41605797,  0.51858836,\n",
              "          0.06519934,  0.9183956 , -0.92177236, -0.32098645, -0.8756186 ,\n",
              "          0.5004361 ,  0.9801905 , -0.91063964,  0.14475182,  0.6583333 ,\n",
              "         -0.18443774,  0.36420593, -0.4240014 ,  0.1619866 ,  0.8620015 ,\n",
              "         -0.33198404,  0.1990656 , -0.4081088 , -0.07254337, -0.33869258,\n",
              "         -0.30839103, -0.64051986, -0.61156255,  0.7253657 , -0.2888664 ,\n",
              "          0.3900304 ,  0.9067333 , -0.03222425, -0.08755511, -0.07373793,\n",
              "         -0.65570635, -0.7272714 , -0.5967959 ,  0.07235501,  0.26347804,\n",
              "          0.5225863 , -0.29539654,  0.992342  ,  0.3983035 , -0.2544526 ,\n",
              "         -0.28120327, -0.45584625,  0.47195113, -0.8864588 , -0.50867313,\n",
              "         -0.30237466,  0.43593967,  0.23287667,  0.9999959 , -0.8411575 ,\n",
              "         -0.8716144 , -0.63739634, -0.24933548,  0.58289844, -0.1031876 ,\n",
              "         -1.        ,  0.22786607, -0.77909964,  0.8523781 , -0.7560842 ,\n",
              "          0.9448525 , -0.66515756, -0.36977026, -0.36001983,  0.88523424,\n",
              "          0.89223564, -0.366456  , -0.27651444,  0.70607847, -0.5325352 ,\n",
              "          0.9648017 ,  0.19735828, -0.13097401,  0.21240462,  0.7766575 ,\n",
              "         -0.9021658 , -0.48400053,  0.52324843]], dtype=float32)>,\n",
              " 'sequence_output': <tf.Tensor: shape=(1, 128, 768), dtype=float32, numpy=\n",
              " array([[[-5.5853043e-02, -1.9755675e-01,  6.4958468e-02, ...,\n",
              "          -2.1587220e-01,  5.9806556e-01,  4.3526551e-01],\n",
              "         [ 4.8271894e-02, -1.9900392e-01, -5.0419092e-01, ...,\n",
              "          -8.5937046e-02,  2.7296653e-01,  2.9258022e-01],\n",
              "         [ 8.3109325e-01,  2.0109113e-02, -1.1282250e-02, ...,\n",
              "           5.2618271e-01, -1.0882966e-02,  1.8892158e-04],\n",
              "         ...,\n",
              "         [-2.6907486e-01, -3.0361760e-01,  1.4330816e-01, ...,\n",
              "          -6.7106321e-02, -8.0915833e-01,  3.7342048e-01],\n",
              "         [-8.0237484e-01,  2.0807880e-01, -4.6244180e-01, ...,\n",
              "          -2.6407379e-01, -6.3195539e-01, -4.3557918e-01],\n",
              "         [-5.1775598e-03,  3.0852658e-01,  3.1194410e-01, ...,\n",
              "           4.1307107e-01, -2.5533882e-01, -1.1421091e-01]]], dtype=float32)>}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model**"
      ],
      "metadata": {
        "id": "hLbwe99Sc9_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import activations\n",
        "\n",
        "inputs = keras.layers.Input(shape=(), dtype=tf.string, name=\"inputs\")\n",
        "preprocess = pre_processor(inputs)\n",
        "encode = encode_input(preprocess)\n",
        "\n",
        "nn1 = keras.layers.Dropout(0.1, name=\"dropout\")(encode[\"pooled_output\"])\n",
        "nn1 = keras.layers.Dense(1, activation=keras.activations.sigmoid, name=\"output\")(nn1)\n",
        "\n",
        "model = keras.Model(inputs=[inputs], outputs=[nn1])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CQYTaeBxc7_l",
        "outputId": "e84d4693-fae3-4804-862b-58deb4498bc4"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " inputs (InputLayer)            [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " keras_layer (KerasLayer)       {'input_word_ids':   0           ['inputs[0][0]']                 \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_type_ids':                                                \n",
            "                                (None, 128),                                                      \n",
            "                                 'input_mask': (Non                                               \n",
            "                                e, 128)}                                                          \n",
            "                                                                                                  \n",
            " keras_layer_1 (KerasLayer)     {'pooled_output': (  109482241   ['keras_layer[0][0]',            \n",
            "                                None, 768),                       'keras_layer[0][1]',            \n",
            "                                 'default': (None,                'keras_layer[0][2]']            \n",
            "                                768),                                                             \n",
            "                                 'sequence_output':                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 'encoder_outputs':                                               \n",
            "                                 [(None, 128, 768),                                               \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768),                                                \n",
            "                                 (None, 128, 768)]}                                               \n",
            "                                                                                                  \n",
            " dropout (Dropout)              (None, 768)          0           ['keras_layer_1[0][13]']         \n",
            "                                                                                                  \n",
            " output (Dense)                 (None, 1)            769         ['dropout[0][0]']                \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 109,483,010\n",
            "Trainable params: 769\n",
            "Non-trainable params: 109,482,241\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compile Model\n",
        "METRICS = [\n",
        "           tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"),\n",
        "           tf.keras.metrics.Precision(name=\"precision\"),\n",
        "           tf.keras.metrics.Recall(name=\"recall\"),\n",
        "]\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
        "              loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "              metrics=METRICS)"
      ],
      "metadata": {
        "id": "EkWOazZ9gGb7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfF3VCb8gLBI",
        "outputId": "5e6dffc6-3190-4ca9-cc9e-ed85c5ffd3df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "  2/782 [..............................] - ETA: 3:03:29 - loss: 0.7197 - accuracy: 0.4844 - precision: 0.5278 - recall: 0.5429"
          ]
        }
      ]
    }
  ]
}